---
layout: post
title:  "Kafkaæºç "
date:   2022-05-10 15:40:06 +0800--
categories: [Kafka]
tags: [Kafka, ]  

---

# å‰è¨€

æœ¬æ–‡é˜…è¯»çš„Kafkaæºç ç‰ˆæœ¬ä¸ºï¼š[kafka-3.0.0](http://kafka.apache.org/downloads)



# ç”Ÿäº§è€…æºç 

## ç”Ÿäº§è€…æ¶ˆæ¯å‘é€æµç¨‹

åœ¨æ¶ˆæ¯å‘é€çš„è¿‡ç¨‹ä¸­ï¼Œæ¶‰åŠåˆ°äº†ä¸¤ä¸ªçº¿ç¨‹ï¼š**main** çº¿ç¨‹å’Œ **Sender** çº¿ç¨‹ã€‚åœ¨ main çº¿ç¨‹ ä¸­åˆ›å»ºäº†ä¸€ä¸ªåŒç«¯é˜Ÿåˆ— **RecordAccumulator**ã€‚main çº¿ç¨‹é€šè¿‡åˆ†åŒºå™¨å°†æ¶ˆæ¯å‘é€ç»™ RecordAccumulatorï¼Œ Sender çº¿ç¨‹ä¸æ–­ä» RecordAccumulator ä¸­æ‹‰å–æ¶ˆæ¯å‘é€åˆ° Kafka Brokerã€‚

![image-20220426144944892](/assets/imgs/image-20220426144944892.png)

- **batch.size**ï¼šåªæœ‰æ•°æ®ç§¯ç´¯åˆ°batch.sizeä¹‹åï¼Œsenderæ‰ä¼šå‘é€æ•°æ®ï¼Œé»˜è®¤16k 
- **linger.ms**ï¼šå¦‚æœæ•°æ®è¿Ÿè¿Ÿæœªè¾¾åˆ°batch.size,senderç­‰å¾…linger.msè®¾ç½®çš„æ—¶é—´ã€‚åˆ°äº†ä¹‹åå°±ä¼šå‘é€æ•°æ®ï¼Œå•ä½msï¼Œé»˜è®¤å€¼æ˜¯0msï¼Œè¡¨ç¤ºæ²¡æœ‰å»¶è¿Ÿã€‚

åº”ç­”acksï¼š

- 0ï¼šç”Ÿäº§è€…å‘é€è¿‡æ¥çš„æ•°æ®ï¼Œä¸éœ€è¦ç­‰æ•°æ®è½ç›˜åº”ç­”ã€‚
- 1ï¼šç”Ÿäº§è€…å‘é€è¿‡æ¥çš„æ•°æ®ï¼ŒLeader æ”¶åˆ°æ•°æ®ååº”ç­”ã€‚
- -1(all)ï¼šç”Ÿäº§è€…å‘é€è¿‡æ¥çš„æ•°æ®ï¼ŒLeader å’Œ ISR é˜Ÿåˆ— é‡Œé¢çš„æ‰€æœ‰èŠ‚ç‚¹æ”¶é½æ•°æ®ååº”ç­”ã€‚-1å’Œ all ç­‰ä»·ã€‚



## åˆå§‹åŒ–

### ç”Ÿäº§è€…mainçº¿ç¨‹åˆå§‹åŒ–

![image-20220510193208498](/assets/imgs/image-20220510193208498.png)



1ï¼‰mainçº¿ç¨‹ä¸­é¦–å…ˆä¼šé€šè¿‡æ„é€ å™¨åˆ›å»ºä¸€ä¸ª`KafkaProducer()`

```java
public KafkaProducer(Properties properties) {
  this(properties, null, null);
}
// ç„¶åä¼šä¾æ¬¡è°ƒç”¨ä»¥ä¸‹æ„é€ å™¨
public KafkaProducer(Properties properties, Serializer<K> keySerializer, Serializer<V> valueSerializer) {
  this(Utils.propsToMap(properties), keySerializer, valueSerializer);
}
public KafkaProducer(Map<String, Object> configs, Serializer<K> keySerializer, Serializer<V> valueSerializer) {
  this(new ProducerConfig(ProducerConfig.appendSerializerToConfig(configs, keySerializer, valueSerializer)),
       keySerializer, valueSerializer, null, null, null, Time.SYSTEM);
}
```



2ï¼‰æœ€ç»ˆè°ƒç”¨çš„KafkaProduceræ„é€ å™¨

```java
@SuppressWarnings("unchecked")
KafkaProducer(ProducerConfig config,
              Serializer<K> keySerializer,
              Serializer<V> valueSerializer,
              ProducerMetadata metadata,
              KafkaClient kafkaClient,
              ProducerInterceptors<K, V> interceptors,
              Time time) {
  try {
    this.producerConfig = config;
    this.time = time;

    // è·å–äº‹åŠ¡id
    String transactionalId = config.getString(ProducerConfig.TRANSACTIONAL_ID_CONFIG);

    // è·å–å®¢æˆ·ç«¯id
    this.clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);

    LogContext logContext;
    if (transactionalId == null)
      logContext = new LogContext(String.format("[Producer clientId=%s] ", clientId));
    else
      logContext = new LogContext(String.format("[Producer clientId=%s, transactionalId=%s] ", clientId, transactionalId));
    log = logContext.logger(KafkaProducer.class);
    log.trace("Starting the Kafka producer");

    Map<String, String> metricTags = Collections.singletonMap("client-id", clientId);
    MetricConfig metricConfig = new MetricConfig().samples(config.getInt(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG))
      .timeWindow(config.getLong(ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS)
      .recordLevel(Sensor.RecordingLevel.forName(config.getString(ProducerConfig.METRICS_RECORDING_LEVEL_CONFIG)))
      .tags(metricTags);
    List<MetricsReporter> reporters = config.getConfiguredInstances(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,
                                                                    MetricsReporter.class,
                                                                    Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
    // ç›‘æ§kafkaè¿è¡Œæƒ…å†µ
    JmxReporter jmxReporter = new JmxReporter();
    jmxReporter.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)));
    reporters.add(jmxReporter);
    MetricsContext metricsContext = new KafkaMetricsContext(JMX_PREFIX,
                                                            config.originalsWithPrefix(CommonClientConfigs.METRICS_CONTEXT_PREFIX));
    this.metrics = new Metrics(metricConfig, reporters, time, metricsContext);
    // è·å–åˆ†åŒºå™¨
    this.partitioner = config.getConfiguredInstance(
      ProducerConfig.PARTITIONER_CLASS_CONFIG,
      Partitioner.class,
      Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
    // é‡è¯•æ—¶é—´é—´éš”å‚æ•°é…ç½®ï¼Œé»˜è®¤å€¼100ms
    long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);
    // keyå’Œvalueçš„åºåˆ—åŒ–
    if (keySerializer == null) {
      this.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                                                        Serializer.class);
      this.keySerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), true);
    } else {
      config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);
      this.keySerializer = keySerializer;
    }
    if (valueSerializer == null) {
      this.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                                                          Serializer.class);
      this.valueSerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), false);
    } else {
      config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);
      this.valueSerializer = valueSerializer;
    }

    // æ‹¦æˆªå™¨å¤„ç†ï¼ˆæ‹¦æˆªå™¨å¯ä»¥æœ‰å¤šä¸ªï¼‰
    List<ProducerInterceptor<K, V>> interceptorList = (List) config.getConfiguredInstances(
      ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,
      ProducerInterceptor.class,
      Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
    if (interceptors != null)
      this.interceptors = interceptors;
    else
      this.interceptors = new ProducerInterceptors<>(interceptorList);
    ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keySerializer,
                                                                                          valueSerializer, interceptorList, reporters);
    // å•æ¡æ—¥å¿—å¤§å° é»˜è®¤1m
    this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);
    // ç¼“å†²åŒºå¤§å° é»˜è®¤32m
    this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);
    // å‹ç¼©ï¼Œé»˜è®¤æ˜¯none
    this.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));

    this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);
    int deliveryTimeoutMs = configureDeliveryTimeout(config, log);

    this.apiVersions = new ApiVersions();
    this.transactionManager = configureTransactionState(config, logContext);
    // ç¼“å†²åŒºå¯¹è±¡ é»˜è®¤æ˜¯32m å‚æ•°åˆ—è¡¨å¦‚ä¸‹ï¼š
    // æ‰¹æ¬¡å¤§å° é»˜è®¤16k
    // å‹ç¼©æ–¹å¼ï¼Œé»˜è®¤æ˜¯none
    // liner.ms é»˜è®¤æ˜¯0
    //  å†…å­˜æ± 
    this.accumulator = new RecordAccumulator(logContext,
                                             config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),
                                             this.compressionType,
                                             lingerMs(config),
                                             retryBackoffMs,
                                             deliveryTimeoutMs,
                                             metrics,
                                             PRODUCER_METRIC_GROUP_NAME,
                                             time,
                                             apiVersions,
                                             transactionManager,
                                             new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));

    // è¿æ¥ä¸Škafkaé›†ç¾¤åœ°å€
    List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
      config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG),
      config.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG));
    // è·å–å…ƒæ•°æ®
    if (metadata != null) {
      this.metadata = metadata;
    } else {
      this.metadata = new ProducerMetadata(retryBackoffMs,
                                           config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),
                                           config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),
                                           logContext,
                                           clusterResourceListeners,
                                           Time.SYSTEM);
      this.metadata.bootstrap(addresses);
    }
    this.errors = this.metrics.sensor("errors");
		
    this.sender = newSender(logContext, kafkaClient, this.metadata);
    String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;
    // æŠŠsenderçº¿ç¨‹æ”¾åˆ°åå°
    this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
    // â­ï¸å¯åŠ¨senderçº¿ç¨‹
    this.ioThread.start();
    config.logUnused();
    AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());
    log.debug("Kafka producer started");
  } catch (Throwable t) {
    // call close methods if internal objects are already constructed this is to prevent resource leak. see KAFKA-2121
    close(Duration.ofMillis(0), true);
    // now propagate the exception
    throw new KafkaException("Failed to construct kafka producer", t);
  }
}
```



---



### ç”Ÿäº§è€…senderçº¿ç¨‹åˆå§‹åŒ–

![image-20220510193229658](/assets/imgs/image-20220510193229658.png)

1ï¼‰mainçº¿ç¨‹åˆå§‹åŒ–ä¸­ï¼Œè°ƒç”¨`newSender(logContext, kafkaClient, this.metadata);`æ¥åˆ°senderçº¿ç¨‹åˆå§‹åŒ–

```java
Sender newSender(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata) {
  // ç¼“å­˜è¯·æ±‚çš„ä¸ªæ•° é»˜è®¤æ˜¯5ä¸ª
  int maxInflightRequests = configureInflightRequests(producerConfig);
  // è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤30s
  int requestTimeoutMs = producerConfig.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);
  ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(producerConfig, time, logContext);
  ProducerMetrics metricsRegistry = new ProducerMetrics(this.metrics);
  Sensor throttleTimeSensor = Sender.throttleTimeSensor(metricsRegistry.senderMetrics);

  // åˆ›å»ºä¸€ä¸ªå®¢æˆ·ç«¯å¯¹è±¡
  // clientId  å®¢æˆ·ç«¯id
  // maxInflightRequests  ç¼“å­˜è¯·æ±‚çš„ä¸ªæ•° é»˜è®¤æ˜¯5ä¸ª
  // RECONNECT_BACKOFF_MS_CONFIG é‡è¯•æ—¶é—´
  // RECONNECT_BACKOFF_MAX_MS_CONFIG æ€»çš„é‡è¯•æ—¶é—´
  // å‘é€ç¼“å†²åŒºå¤§å°send.buffer.bytes  é»˜è®¤128kb
  // æ¥æ”¶æ•°æ®ç¼“å­˜ receive.buffer.bytes é»˜è®¤æ˜¯32kb
  KafkaClient client = kafkaClient != null ? kafkaClient : new NetworkClient(
    new Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),
                 this.metrics, time, "producer", channelBuilder, logContext),
    metadata,
    clientId,
    maxInflightRequests,
    producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),
    producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),
    producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),
    producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),
    requestTimeoutMs,
    producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),
    producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG),
    time,
    true,
    apiVersions,
    throttleTimeSensor,
    logContext);
  // 0 ï¼šç”Ÿäº§è€…å‘é€è¿‡æ¥ï¼Œä¸éœ€è¦åº”ç­”ï¼›  1 ï¼šleaderæ”¶åˆ°ï¼Œåº”ç­”ï¼›  -1 ï¼šleaderå’Œisré˜Ÿåˆ—é‡Œé¢æ‰€æœ‰çš„éƒ½æ”¶åˆ°äº†åº”ç­”
  short acks = configureAcks(producerConfig, log);
  // åˆ›å»ºsenderçº¿ç¨‹
  return new Sender(logContext,
                    client,
                    metadata,
                    this.accumulator,
                    maxInflightRequests == 1,
                    producerConfig.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),
                    acks,
                    producerConfig.getInt(ProducerConfig.RETRIES_CONFIG),
                    metricsRegistry.senderMetrics,
                    time,
                    requestTimeoutMs,
                    producerConfig.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG),
                    this.transactionManager,
                    apiVersions);
}
```



2ï¼‰Sender å¯¹è±¡è¢«æ”¾åˆ°äº†ä¸€ä¸ªçº¿ç¨‹ä¸­å¯åŠ¨ï¼Œæ‰€æœ‰éœ€è¦ç‚¹å‡» `newSender()`æ–¹æ³•ä¸­çš„ Senderï¼Œå¹¶ æ‰¾åˆ° sender å¯¹è±¡ä¸­çš„ `run()`æ–¹æ³•ã€‚

```java
public void run() {
  log.debug("Starting Kafka producer I/O thread.");

  // main loop, runs until close is called
  while (running) {
    try {
      // sender çº¿ç¨‹ä»ç¼“å†²åŒºå‡†å¤‡æ‹‰å–æ•°æ®ï¼Œåˆšå¯åŠ¨æ‹‰ä¸åˆ°æ•°æ®
      runOnce();
    } catch (Exception e) {
      log.error("Uncaught error in kafka producer I/O thread: ", e);
    }
  }
  ...
}
```



## ç”Ÿäº§è€…å‘é€æ•°æ®åˆ°ç¼“å†²åŒº

![image-20220511222901498](/assets/imgs/image-20220511222901498.png)

### å‘é€æ€»ä½“æµç¨‹

1ï¼‰`KafkaProducer.send()`æ–¹æ³•

```java
public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {
  // intercept the record, which can be potentially modified; this method does not throw exceptions
  // æ‹¦æˆªå™¨ç›¸å…³æ“ä½œ
  ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);
  return doSend(interceptedRecord, callback);
}
```



2ï¼‰å…¶ä¸­çš„ `onSend()`æ–¹æ³•ï¼Œè¿›è¡Œæ‹¦æˆªå™¨ç›¸å…³å¤„ç†ã€‚

```java
public ProducerRecord<K, V> onSend(ProducerRecord<K, V> record) {
  ProducerRecord<K, V> interceptRecord = record;
  for (ProducerInterceptor<K, V> interceptor : this.interceptors) {
    try {
      // æ‹¦æˆªå™¨å¯¹æ•°æ®è¿›è¡ŒåŠ å·¥
      interceptRecord = interceptor.onSend(interceptRecord);
    } catch (Exception e) {
      // do not propagate interceptor exception, log and continue calling other interceptors
      // be careful not to throw exception from here
      if (record != null)
        log.warn("Error executing interceptor onSend callback for topic: {}, partition: {}", record.topic(), record.partition(), e);
      else
        log.warn("Error executing interceptor onSend callback", e);
    }
  }
  return interceptRecord;
}
```



3ï¼‰ä»æ‹¦æˆªå™¨å¤„ç†ä¸­è¿”å›ï¼Œç‚¹å‡» `doSend()`æ–¹æ³•ã€‚

```java
private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {
  TopicPartition tp = null;
  try {
    throwIfProducerClosed();
    // first make sure the metadata for the topic is available
    long nowMs = time.milliseconds();
    ClusterAndWaitTime clusterAndWaitTime;
    try {
      // ä» Kafka æ‹‰å–å…ƒæ•°æ®ã€‚maxBlockTimeMs è¡¨ç¤ºæœ€å¤šèƒ½ç­‰å¾…å¤šé•¿æ—¶é—´ã€‚
      clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);
    } catch (KafkaException e) {
      if (metadata.isClosed())
        throw new KafkaException("Producer closed while send in progress", e);
      throw e;
    }
    nowMs += clusterAndWaitTime.waitedOnMetadataMs;
    // å‰©ä½™æ—¶é—´ = æœ€å¤šèƒ½ç­‰å¾…æ—¶é—´ - ç”¨äº†å¤šå°‘æ—¶é—´;
    long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);
    // æ›´æ–°é›†ç¾¤å…ƒæ•°æ®
    Cluster cluster = clusterAndWaitTime.cluster;
    // åºåˆ—åŒ–ç›¸å…³æ“ä½œ
    byte[] serializedKey;
    try {
      serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());
    } catch (ClassCastException cce) {
      throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
                                       " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
                                       " specified in key.serializer", cce);
    }
    byte[] serializedValue;
    try {
      serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());
    } catch (ClassCastException cce) {
      throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
                                       " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
                                       " specified in value.serializer", cce);
    }
    // ğŸ¤”åˆ†åŒºæ“ä½œ(æ ¹æ®å…ƒæ•°æ®ä¿¡æ¯)
    int partition = partition(record, serializedKey, serializedValue, cluster);
    tp = new TopicPartition(record.topic(), partition);

    setReadOnly(record.headers());
    Header[] headers = record.headers().toArray();

    int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),
                                                                       compressionType, serializedKey, serializedValue, headers);
    // ğŸ¤”ä¿è¯æ•°æ®å¤§å°èƒ½å¤Ÿä¼ è¾“(åºåˆ—åŒ–åçš„  å‹ç¼©åçš„)ï¼Œå‘é€æ¶ˆæ¯çš„å¤§å°æ˜¯å¦è¶…è¿‡æœ€å¤§å€¼ï¼Œé»˜è®¤æ˜¯1m
    ensureValidRecordSize(serializedSize);

    long timestamp = record.timestamp() == null ? nowMs : record.timestamp();
    if (log.isTraceEnabled()) {
      log.trace("Attempting to append record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition);
    }
    // æ¶ˆæ¯å‘é€çš„å›è°ƒå‡½æ•°
    // producer callback will make sure to call both 'callback' and interceptor callback
    Callback interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);

    if (transactionManager != null && transactionManager.isTransactional()) {
      transactionManager.failIfNotReadyForSend();
    }
    // ğŸ¤”accumulatorç¼“å­˜  è¿½åŠ æ•°æ®  resultæ˜¯æ˜¯å¦æ·»åŠ æˆåŠŸçš„ç»“æœ
    RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,
                                                                     serializedValue, headers, interceptCallback, remainingWaitMs, true, nowMs);

    if (result.abortForNewBatch) {
      int prevPartition = partition;
      partitioner.onNewBatch(record.topic(), cluster, prevPartition);
      partition = partition(record, serializedKey, serializedValue, cluster);
      tp = new TopicPartition(record.topic(), partition);
      if (log.isTraceEnabled()) {
        log.trace("Retrying append due to new batch creation for topic {} partition {}. The old partition was {}", record.topic(), partition, prevPartition);
      }
      // producer callback will make sure to call both 'callback' and interceptor callback
      interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);

      result = accumulator.append(tp, timestamp, serializedKey,
                                  serializedValue, headers, interceptCallback, remainingWaitMs, false, nowMs);
    }

    if (transactionManager != null && transactionManager.isTransactional())
      transactionManager.maybeAddPartitionToTransaction(tp);
    // æ‰¹æ¬¡å¤§å°å·²ç»æ»¡äº† è·å–æœ‰ä¸€ä¸ªæ–°æ‰¹æ¬¡åˆ›å»º
    if (result.batchIsFull || result.newBatchCreated) {
      log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);
      // å”¤é†’å‘é€çº¿ç¨‹
      this.sender.wakeup();
    }
    return result.future;
    // handling exceptions and record the errors;
    // for API exceptions return them in the future,
    // for other exceptions throw directly
  } catch (ApiException e) {
    log.debug("Exception occurred during message send:", e);
    if (callback != null)
      callback.onCompletion(null, e);
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    return new FutureFailure(e);
  } catch (InterruptedException e) {
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    throw new InterruptException(e);
  } catch (KafkaException e) {
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    throw e;
  } catch (Exception e) {
    // we notify interceptor about all exceptions, since onSend is called before anything else in this method
    this.interceptors.onSendError(record, tp, e);
    throw e;
  }
}
```



### åˆ†åŒºé€‰æ‹©

1ï¼‰å‘é€æ€»ä½“æµç¨‹ä¸­ä»¥ä¸‹ä»£ç ä¸ºåˆ†åŒºé€‰æ‹©ç›¸å…³æµç¨‹ï¼š

```java
// åˆ†åŒºæ“ä½œ
int partition = partition(record, serializedKey, serializedValue, cluster);
tp = new TopicPartition(record.topic(), partition);

// partition()æ–¹æ³•å…·ä½“å¦‚ä¸‹ï¼š
private int partition(ProducerRecord<K, V> record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) {
  Integer partition = record.partition();
  // å¦‚æœæŒ‡å®šåˆ†åŒºï¼ŒæŒ‰ç…§æŒ‡å®šåˆ†åŒºé…ç½®
  return partition != null ?
    partition :
  // åˆ†åŒºå™¨é€‰æ‹©åˆ†åŒº
  partitioner.partition( // 2âƒ£ï¸ ç‚¹å‡» partitionï¼Œè·³è½¬åˆ° Partitioner æ¥å£ï¼Œé€‰æ‹©é»˜è®¤çš„åˆ†åŒºå™¨ DefaultPartitioner
    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);
}
```



2ï¼‰ç‚¹å‡» partitionï¼Œè·³è½¬åˆ° Partitioner æ¥å£ï¼Œé€‰æ‹©é»˜è®¤çš„åˆ†åŒºå™¨ DefaultPartitioner

```java
public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster,
                     int numPartitions) {
  // æ²¡æœ‰æŒ‡å®škey
  if (keyBytes == null) {
    // 3âƒ£ï¸æŒ‰ç…§ç²˜æ€§åˆ†åŒºå¤„ç†
    return stickyPartitionCache.partition(topic, cluster);
  }
  // å¦‚æœæŒ‡å®škey,æŒ‰ç…§keyçš„hashcodeå€¼ å¯¹åˆ†åŒºæ•°æ±‚æ¨¡
  // hash the keyBytes to choose a partition
  return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
}
---------------------------------------------------------
// 3âƒ£ï¸æ²¡æœ‰æŒ‡å®škeyå’Œåˆ†åŒºçš„å¤„ç†æ–¹å¼ stickyPartitionCache.partition(topic, cluster);
public int partition(String topic, Cluster cluster) {
  Integer part = indexCache.get(topic);
  if (part == null) {
    return nextPartition(topic, cluster, -1); // 4âƒ£ï¸
  }
  return part;
}
---------------------------------------------------------
// 4âƒ£ï¸nextPartition(topic, cluster, -1);
public int nextPartition(String topic, Cluster cluster, int prevPartition) {
  List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
  Integer oldPart = indexCache.get(topic);
  Integer newPart = oldPart;
  // Check that the current sticky partition for the topic is either not set or that the partition that 
  // triggered the new batch matches the sticky partition that needs to be changed.
  if (oldPart == null || oldPart == prevPartition) {
    List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);
    if (availablePartitions.size() < 1) {
      Integer random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
      newPart = random % partitions.size();
    } else if (availablePartitions.size() == 1) {
      newPart = availablePartitions.get(0).partition();
    } else {
      while (newPart == null || newPart.equals(oldPart)) {
        int random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
        newPart = availablePartitions.get(random % availablePartitions.size()).partition();
      }
    }
    // Only change the sticky partition if it is null or prevPartition matches the current sticky partition.
    if (oldPart == null) {
      indexCache.putIfAbsent(topic, newPart);
    } else {
      indexCache.replace(topic, prevPartition, newPart);
    }
    return indexCache.get(topic);
  }
  return indexCache.get(topic);
}
```



### å‘é€æ¶ˆæ¯å¤§å°æ ¡éªŒ

å‘é€æ€»ä½“æµç¨‹ä¸­çš„ensureValidRecordSize(serializedSize)æ–¹æ³•æ¶‰åŠåˆ°å‘é€æ¶ˆæ¯å¤§å°æ ¡éªŒæ“ä½œ

```java
private void ensureValidRecordSize(int size) {
  // å•æ¡ä¿¡æ¯æœ€å¤§å€¼ maxRequestSize 1m
  if (size > maxRequestSize)
    throw new RecordTooLargeException("The message is " + size +
                                      " bytes when serialized which is larger than " + maxRequestSize + ", which is the value of the " +
                                      ProducerConfig.MAX_REQUEST_SIZE_CONFIG + " configuration.");
  // ç¼“å†²åŒºå†…å­˜æ€»å¤§å°ï¼Œé»˜è®¤32m
  if (size > totalMemorySize)
    throw new RecordTooLargeException("The message is " + size +
                                      " bytes when serialized which is larger than the total memory buffer you have configured with the " +
                                      ProducerConfig.BUFFER_MEMORY_CONFIG +
                                      " configuration.");
}
```





### å†…å­˜/ç¼“å­˜æ± 

å‘é€æ€»ä½“æµç¨‹ä¸­çš„`RecordAccumulator.RecordAppendResult result = accumulator.append(...);`

```java
public RecordAppendResult append(TopicPartition tp,
                                 long timestamp,
                                 byte[] key,
                                 byte[] value,
                                 Header[] headers,
                                 Callback callback,
                                 long maxTimeToBlock,
                                 boolean abortOnNewBatch,
                                 long nowMs) throws InterruptedException {
  // We keep track of the number of appending thread to make sure we do not miss batches in
  // abortIncompleteBatches().
  appendsInProgress.incrementAndGet();
  ByteBuffer buffer = null;
  if (headers == null) headers = Record.EMPTY_HEADERS;
  try {
    // check if we have an in-progress batch
    // è·å–æˆ–è€…åˆ›å»ºä¸€ä¸ªé˜Ÿåˆ—ï¼ˆæŒ‰ç…§æ¯ä¸ªä¸»é¢˜çš„åˆ†åŒºï¼‰
    Deque<ProducerBatch> dq = getOrCreateDeque(tp);
    synchronized (dq) {
      if (closed)
        throw new KafkaException("Producer closed while send in progress");
      // å°è¯•å‘é˜Ÿåˆ—é‡Œé¢æ·»åŠ æ•°æ®(æ²¡æœ‰åˆ†é…å†…å­˜ã€æ‰¹æ¬¡å¯¹è±¡ï¼Œæ‰€ä»¥å¤±è´¥)
      RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);
      if (appendResult != null)
        return appendResult;
    }

    // we don't have an in-progress record batch try to allocate a new batch
    if (abortOnNewBatch) {
      // Return a result that will cause another call to append.
      return new RecordAppendResult(null, false, false, true);
    }

    byte maxUsableMagic = apiVersions.maxUsableProduceMagic();
    // this.batchSize é»˜è®¤16k    æ•°æ®å¤§å°17k
    int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));
    log.trace("Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms", size, tp.topic(), tp.partition(), maxTimeToBlock);
    // ç”³è¯·å†…å­˜  å†…å­˜æ± æ ¹æ®æ‰¹æ¬¡å¤§å°(é»˜è®¤16k)å’Œæ¶ˆæ¯å¤§å°ä¸­æœ€å¤§å€¼ï¼Œåˆ†é…å†…å­˜   ğŸ˜åŒç«¯é˜Ÿåˆ— 
    buffer = free.allocate(size, maxTimeToBlock);

    // Update the current time in case the buffer allocation blocked above.
    nowMs = time.milliseconds();
    synchronized (dq) {
      // Need to check if producer is closed again after grabbing the dequeue lock.
      if (closed)
        throw new KafkaException("Producer closed while send in progress");
      // å°è¯•å‘é˜Ÿåˆ—é‡Œé¢æ·»åŠ æ•°æ®(æœ‰å†…å­˜ï¼Œä½†æ˜¯æ²¡æœ‰æ‰¹æ¬¡å¯¹è±¡)
      RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);
      if (appendResult != null) {
        // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often...
        return appendResult;
      }
      // å°è£…å†…å­˜buffer
      MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);
      // æ ¹æ®å†…å­˜å¤§å°å°è£…æ‰¹æ¬¡(æœ‰å†…å­˜ã€æœ‰æ‰¹æ¬¡å¯¹è±¡)
      ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, nowMs);
      // å°è¯•å‘é˜Ÿåˆ—é‡Œé¢æ·»åŠ æ•°æ®
      FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers,
                                                                           callback, nowMs));
      // æŠŠæ–°åˆ›å»ºçš„æ‰¹æ¬¡æ”¾åˆ°é˜Ÿåˆ—æœ«å°¾
      dq.addLast(batch);
      incomplete.add(batch);

      // Don't deallocate this buffer in the finally block as it's being used in the record batch
      buffer = null;
      return new RecordAppendResult(future, dq.size() > 1 || batch.isFull(), true, false);
    }
  } finally {
    // å¦‚æœå‘ç”Ÿå¼‚å¸¸ï¼Œé‡Šæ”¾å†…å­˜
    if (buffer != null)
      free.deallocate(buffer);
    appendsInProgress.decrementAndGet();
  }
}
```



## **sender** çº¿ç¨‹å‘é€æ•°æ®

![image-20220512000146708](/assets/imgs/image-20220512000146708.png)



