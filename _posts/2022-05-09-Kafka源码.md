---
layout: post
title:  "Kafkaæºç "
date:   2022-05-10 15:40:06 +0800--
categories: [æ¶ˆæ¯é˜Ÿåˆ—, ]
tags: [Kafka, ]  

---

# å‰è¨€

æœ¬æ–‡é˜…è¯»çš„Kafkaæºç ç‰ˆæœ¬ä¸ºï¼š[kafka-3.0.0](http://kafka.apache.org/downloads)



# ç”Ÿäº§è€…æºç 

## ç”Ÿäº§è€…æ¶ˆæ¯å‘é€æµç¨‹

åœ¨æ¶ˆæ¯å‘é€çš„è¿‡ç¨‹ä¸­ï¼Œæ¶‰åŠåˆ°äº†ä¸¤ä¸ªçº¿ç¨‹ï¼š**main** çº¿ç¨‹å’Œ **Sender** çº¿ç¨‹ã€‚åœ¨ main çº¿ç¨‹ ä¸­åˆ›å»ºäº†ä¸€ä¸ªåŒç«¯é˜Ÿåˆ— **RecordAccumulator**ã€‚main çº¿ç¨‹é€šè¿‡åˆ†åŒºå™¨å°†æ¶ˆæ¯å‘é€ç»™ RecordAccumulatorï¼Œ Sender çº¿ç¨‹ä¸æ–­ä» RecordAccumulator ä¸­æ‹‰å–æ¶ˆæ¯å‘é€åˆ° Kafka Brokerã€‚

![image-20220426144944892](/assets/imgs/image-20220426144944892.png)

- **batch.size**ï¼šåªæœ‰æ•°æ®ç§¯ç´¯åˆ°batch.sizeä¹‹åï¼Œsenderæ‰ä¼šå‘é€æ•°æ®ï¼Œé»˜è®¤16k 
- **linger.ms**ï¼šå¦‚æœæ•°æ®è¿Ÿè¿Ÿæœªè¾¾åˆ°batch.size,senderç­‰å¾…linger.msè®¾ç½®çš„æ—¶é—´ã€‚åˆ°äº†ä¹‹åå°±ä¼šå‘é€æ•°æ®ï¼Œå•ä½msï¼Œé»˜è®¤å€¼æ˜¯0msï¼Œè¡¨ç¤ºæ²¡æœ‰å»¶è¿Ÿã€‚

åº”ç­”acksï¼š

- 0ï¼šç”Ÿäº§è€…å‘é€è¿‡æ¥çš„æ•°æ®ï¼Œä¸éœ€è¦ç­‰æ•°æ®è½ç›˜åº”ç­”ã€‚
- 1ï¼šç”Ÿäº§è€…å‘é€è¿‡æ¥çš„æ•°æ®ï¼ŒLeader æ”¶åˆ°æ•°æ®ååº”ç­”ã€‚
- -1(all)ï¼šç”Ÿäº§è€…å‘é€è¿‡æ¥çš„æ•°æ®ï¼ŒLeader å’Œ ISR é˜Ÿåˆ— é‡Œé¢çš„æ‰€æœ‰èŠ‚ç‚¹æ”¶é½æ•°æ®ååº”ç­”ã€‚-1å’Œ all ç­‰ä»·ã€‚



## åˆå§‹åŒ–

### ç”Ÿäº§è€…mainçº¿ç¨‹åˆå§‹åŒ–

![image-20220510193208498](/assets/imgs/image-20220510193208498.png)



1ï¼‰mainçº¿ç¨‹ä¸­é¦–å…ˆä¼šé€šè¿‡æ„é€ å™¨åˆ›å»ºä¸€ä¸ª`KafkaProducer()`

```java
public KafkaProducer(Properties properties) {
  this(properties, null, null);
}
// ç„¶åä¼šä¾æ¬¡è°ƒç”¨ä»¥ä¸‹æ„é€ å™¨
public KafkaProducer(Properties properties, Serializer<K> keySerializer, Serializer<V> valueSerializer) {
  this(Utils.propsToMap(properties), keySerializer, valueSerializer);
}
public KafkaProducer(Map<String, Object> configs, Serializer<K> keySerializer, Serializer<V> valueSerializer) {
  this(new ProducerConfig(ProducerConfig.appendSerializerToConfig(configs, keySerializer, valueSerializer)),
       keySerializer, valueSerializer, null, null, null, Time.SYSTEM);
}
```



2ï¼‰æœ€ç»ˆè°ƒç”¨çš„KafkaProduceræ„é€ å™¨

```java
@SuppressWarnings("unchecked")
KafkaProducer(ProducerConfig config,
              Serializer<K> keySerializer,
              Serializer<V> valueSerializer,
              ProducerMetadata metadata,
              KafkaClient kafkaClient,
              ProducerInterceptors<K, V> interceptors,
              Time time) {
  try {
    this.producerConfig = config;
    this.time = time;

    // è·å–äº‹åŠ¡id
    String transactionalId = config.getString(ProducerConfig.TRANSACTIONAL_ID_CONFIG);

    // è·å–å®¢æˆ·ç«¯id
    this.clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);

    LogContext logContext;
    if (transactionalId == null)
      logContext = new LogContext(String.format("[Producer clientId=%s] ", clientId));
    else
      logContext = new LogContext(String.format("[Producer clientId=%s, transactionalId=%s] ", clientId, transactionalId));
    log = logContext.logger(KafkaProducer.class);
    log.trace("Starting the Kafka producer");

    Map<String, String> metricTags = Collections.singletonMap("client-id", clientId);
    MetricConfig metricConfig = new MetricConfig().samples(config.getInt(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG))
      .timeWindow(config.getLong(ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS)
      .recordLevel(Sensor.RecordingLevel.forName(config.getString(ProducerConfig.METRICS_RECORDING_LEVEL_CONFIG)))
      .tags(metricTags);
    List<MetricsReporter> reporters = config.getConfiguredInstances(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,
                                                                    MetricsReporter.class,
                                                                    Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
    // ç›‘æ§kafkaè¿è¡Œæƒ…å†µ
    JmxReporter jmxReporter = new JmxReporter();
    jmxReporter.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)));
    reporters.add(jmxReporter);
    MetricsContext metricsContext = new KafkaMetricsContext(JMX_PREFIX,
                                                            config.originalsWithPrefix(CommonClientConfigs.METRICS_CONTEXT_PREFIX));
    this.metrics = new Metrics(metricConfig, reporters, time, metricsContext);
    // è·å–åˆ†åŒºå™¨
    this.partitioner = config.getConfiguredInstance(
      ProducerConfig.PARTITIONER_CLASS_CONFIG,
      Partitioner.class,
      Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
    // é‡è¯•æ—¶é—´é—´éš”å‚æ•°é…ç½®ï¼Œé»˜è®¤å€¼100ms
    long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);
    // keyå’Œvalueçš„åºåˆ—åŒ–
    if (keySerializer == null) {
      this.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                                                        Serializer.class);
      this.keySerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), true);
    } else {
      config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);
      this.keySerializer = keySerializer;
    }
    if (valueSerializer == null) {
      this.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                                                          Serializer.class);
      this.valueSerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), false);
    } else {
      config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);
      this.valueSerializer = valueSerializer;
    }

    // æ‹¦æˆªå™¨å¤„ç†ï¼ˆæ‹¦æˆªå™¨å¯ä»¥æœ‰å¤šä¸ªï¼‰
    List<ProducerInterceptor<K, V>> interceptorList = (List) config.getConfiguredInstances(
      ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,
      ProducerInterceptor.class,
      Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
    if (interceptors != null)
      this.interceptors = interceptors;
    else
      this.interceptors = new ProducerInterceptors<>(interceptorList);
    ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keySerializer,
                                                                                          valueSerializer, interceptorList, reporters);
    // å•æ¡æ—¥å¿—å¤§å° é»˜è®¤1m
    this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);
    // ç¼“å†²åŒºå¤§å° é»˜è®¤32m
    this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);
    // å‹ç¼©ï¼Œé»˜è®¤æ˜¯none
    this.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));

    this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);
    int deliveryTimeoutMs = configureDeliveryTimeout(config, log);

    this.apiVersions = new ApiVersions();
    this.transactionManager = configureTransactionState(config, logContext);
    // ç¼“å†²åŒºå¯¹è±¡ é»˜è®¤æ˜¯32m å‚æ•°åˆ—è¡¨å¦‚ä¸‹ï¼š
    // æ‰¹æ¬¡å¤§å° é»˜è®¤16k
    // å‹ç¼©æ–¹å¼ï¼Œé»˜è®¤æ˜¯none
    // liner.ms é»˜è®¤æ˜¯0
    //  å†…å­˜æ± 
    this.accumulator = new RecordAccumulator(logContext,
                                             config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),
                                             this.compressionType,
                                             lingerMs(config),
                                             retryBackoffMs,
                                             deliveryTimeoutMs,
                                             metrics,
                                             PRODUCER_METRIC_GROUP_NAME,
                                             time,
                                             apiVersions,
                                             transactionManager,
                                             new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));

    // è¿æ¥ä¸Škafkaé›†ç¾¤åœ°å€
    List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
      config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG),
      config.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG));
    // è·å–å…ƒæ•°æ®
    if (metadata != null) {
      this.metadata = metadata;
    } else {
      this.metadata = new ProducerMetadata(retryBackoffMs,
                                           config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),
                                           config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),
                                           logContext,
                                           clusterResourceListeners,
                                           Time.SYSTEM);
      this.metadata.bootstrap(addresses);
    }
    this.errors = this.metrics.sensor("errors");
		
    this.sender = newSender(logContext, kafkaClient, this.metadata);
    String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;
    // æŠŠsenderçº¿ç¨‹æ”¾åˆ°åå°
    this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
    // â­ï¸å¯åŠ¨senderçº¿ç¨‹
    this.ioThread.start();
    config.logUnused();
    AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());
    log.debug("Kafka producer started");
  } catch (Throwable t) {
    // call close methods if internal objects are already constructed this is to prevent resource leak. see KAFKA-2121
    close(Duration.ofMillis(0), true);
    // now propagate the exception
    throw new KafkaException("Failed to construct kafka producer", t);
  }
}
```



---



### ç”Ÿäº§è€…senderçº¿ç¨‹åˆå§‹åŒ–

![image-20220510193229658](/assets/imgs/image-20220510193229658.png)

1ï¼‰mainçº¿ç¨‹åˆå§‹åŒ–ä¸­ï¼Œè°ƒç”¨`newSender(logContext, kafkaClient, this.metadata);`æ¥åˆ°senderçº¿ç¨‹åˆå§‹åŒ–

```java
Sender newSender(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata) {
  // ç¼“å­˜è¯·æ±‚çš„ä¸ªæ•° é»˜è®¤æ˜¯5ä¸ª
  int maxInflightRequests = configureInflightRequests(producerConfig);
  // è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤30s
  int requestTimeoutMs = producerConfig.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);
  ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(producerConfig, time, logContext);
  ProducerMetrics metricsRegistry = new ProducerMetrics(this.metrics);
  Sensor throttleTimeSensor = Sender.throttleTimeSensor(metricsRegistry.senderMetrics);

  // åˆ›å»ºä¸€ä¸ªå®¢æˆ·ç«¯å¯¹è±¡
  // clientId  å®¢æˆ·ç«¯id
  // maxInflightRequests  ç¼“å­˜è¯·æ±‚çš„ä¸ªæ•° é»˜è®¤æ˜¯5ä¸ª
  // RECONNECT_BACKOFF_MS_CONFIG é‡è¯•æ—¶é—´
  // RECONNECT_BACKOFF_MAX_MS_CONFIG æ€»çš„é‡è¯•æ—¶é—´
  // å‘é€ç¼“å†²åŒºå¤§å°send.buffer.bytes  é»˜è®¤128kb
  // æ¥æ”¶æ•°æ®ç¼“å­˜ receive.buffer.bytes é»˜è®¤æ˜¯32kb
  KafkaClient client = kafkaClient != null ? kafkaClient : new NetworkClient(
    new Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),
                 this.metrics, time, "producer", channelBuilder, logContext),
    metadata,
    clientId,
    maxInflightRequests,
    producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),
    producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),
    producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),
    producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),
    requestTimeoutMs,
    producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),
    producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG),
    time,
    true,
    apiVersions,
    throttleTimeSensor,
    logContext);
  // 0 ï¼šç”Ÿäº§è€…å‘é€è¿‡æ¥ï¼Œä¸éœ€è¦åº”ç­”ï¼›  1 ï¼šleaderæ”¶åˆ°ï¼Œåº”ç­”ï¼›  -1 ï¼šleaderå’Œisré˜Ÿåˆ—é‡Œé¢æ‰€æœ‰çš„éƒ½æ”¶åˆ°äº†åº”ç­”
  short acks = configureAcks(producerConfig, log);
  // åˆ›å»ºsenderçº¿ç¨‹
  return new Sender(logContext,
                    client,
                    metadata,
                    this.accumulator,
                    maxInflightRequests == 1,
                    producerConfig.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),
                    acks,
                    producerConfig.getInt(ProducerConfig.RETRIES_CONFIG),
                    metricsRegistry.senderMetrics,
                    time,
                    requestTimeoutMs,
                    producerConfig.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG),
                    this.transactionManager,
                    apiVersions);
}
```



2ï¼‰Sender å¯¹è±¡è¢«æ”¾åˆ°äº†ä¸€ä¸ªçº¿ç¨‹ä¸­å¯åŠ¨ï¼Œæ‰€æœ‰éœ€è¦ç‚¹å‡» `newSender()`æ–¹æ³•ä¸­çš„ Senderï¼Œå¹¶ æ‰¾åˆ° sender å¯¹è±¡ä¸­çš„ `run()`æ–¹æ³•ã€‚

```java
public void run() {
  log.debug("Starting Kafka producer I/O thread.");

  // main loop, runs until close is called
  while (running) {
    try {
      // sender çº¿ç¨‹ä»ç¼“å†²åŒºå‡†å¤‡æ‹‰å–æ•°æ®ï¼Œåˆšå¯åŠ¨æ‹‰ä¸åˆ°æ•°æ®
      runOnce();
    } catch (Exception e) {
      log.error("Uncaught error in kafka producer I/O thread: ", e);
    }
  }
  ...
}
```



## ç”Ÿäº§è€…å‘é€æ•°æ®åˆ°ç¼“å†²åŒº

![image-20220511222901498](/assets/imgs/image-20220511222901498.png)

### å‘é€æ€»ä½“æµç¨‹

1ï¼‰`KafkaProducer.send()`æ–¹æ³•

```java
public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {
  // intercept the record, which can be potentially modified; this method does not throw exceptions
  // æ‹¦æˆªå™¨ç›¸å…³æ“ä½œ
  ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);
  return doSend(interceptedRecord, callback);
}
```



2ï¼‰å…¶ä¸­çš„ `onSend()`æ–¹æ³•ï¼Œè¿›è¡Œæ‹¦æˆªå™¨ç›¸å…³å¤„ç†ã€‚

```java
public ProducerRecord<K, V> onSend(ProducerRecord<K, V> record) {
  ProducerRecord<K, V> interceptRecord = record;
  for (ProducerInterceptor<K, V> interceptor : this.interceptors) {
    try {
      // æ‹¦æˆªå™¨å¯¹æ•°æ®è¿›è¡ŒåŠ å·¥
      interceptRecord = interceptor.onSend(interceptRecord);
    } catch (Exception e) {
      // do not propagate interceptor exception, log and continue calling other interceptors
      // be careful not to throw exception from here
      if (record != null)
        log.warn("Error executing interceptor onSend callback for topic: {}, partition: {}", record.topic(), record.partition(), e);
      else
        log.warn("Error executing interceptor onSend callback", e);
    }
  }
  return interceptRecord;
}
```



3ï¼‰ä»æ‹¦æˆªå™¨å¤„ç†ä¸­è¿”å›ï¼Œç‚¹å‡» `doSend()`æ–¹æ³•ã€‚

```java
private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {
  TopicPartition tp = null;
  try {
    throwIfProducerClosed();
    // first make sure the metadata for the topic is available
    long nowMs = time.milliseconds();
    ClusterAndWaitTime clusterAndWaitTime;
    try {
      // ä» Kafka æ‹‰å–å…ƒæ•°æ®ã€‚maxBlockTimeMs è¡¨ç¤ºæœ€å¤šèƒ½ç­‰å¾…å¤šé•¿æ—¶é—´ã€‚
      clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);
    } catch (KafkaException e) {
      if (metadata.isClosed())
        throw new KafkaException("Producer closed while send in progress", e);
      throw e;
    }
    nowMs += clusterAndWaitTime.waitedOnMetadataMs;
    // å‰©ä½™æ—¶é—´ = æœ€å¤šèƒ½ç­‰å¾…æ—¶é—´ - ç”¨äº†å¤šå°‘æ—¶é—´;
    long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);
    // æ›´æ–°é›†ç¾¤å…ƒæ•°æ®
    Cluster cluster = clusterAndWaitTime.cluster;
    // åºåˆ—åŒ–ç›¸å…³æ“ä½œ
    byte[] serializedKey;
    try {
      serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());
    } catch (ClassCastException cce) {
      throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
                                       " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
                                       " specified in key.serializer", cce);
    }
    byte[] serializedValue;
    try {
      serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());
    } catch (ClassCastException cce) {
      throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
                                       " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
                                       " specified in value.serializer", cce);
    }
    // ğŸ¤”åˆ†åŒºæ“ä½œ(æ ¹æ®å…ƒæ•°æ®ä¿¡æ¯)
    int partition = partition(record, serializedKey, serializedValue, cluster);
    tp = new TopicPartition(record.topic(), partition);

    setReadOnly(record.headers());
    Header[] headers = record.headers().toArray();

    int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),
                                                                       compressionType, serializedKey, serializedValue, headers);
    // ğŸ¤”ä¿è¯æ•°æ®å¤§å°èƒ½å¤Ÿä¼ è¾“(åºåˆ—åŒ–åçš„  å‹ç¼©åçš„)ï¼Œå‘é€æ¶ˆæ¯çš„å¤§å°æ˜¯å¦è¶…è¿‡æœ€å¤§å€¼ï¼Œé»˜è®¤æ˜¯1m
    ensureValidRecordSize(serializedSize);

    long timestamp = record.timestamp() == null ? nowMs : record.timestamp();
    if (log.isTraceEnabled()) {
      log.trace("Attempting to append record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition);
    }
    // æ¶ˆæ¯å‘é€çš„å›è°ƒå‡½æ•°
    // producer callback will make sure to call both 'callback' and interceptor callback
    Callback interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);

    if (transactionManager != null && transactionManager.isTransactional()) {
      transactionManager.failIfNotReadyForSend();
    }
    // ğŸ¤”accumulatorç¼“å­˜  è¿½åŠ æ•°æ®  resultæ˜¯æ˜¯å¦æ·»åŠ æˆåŠŸçš„ç»“æœ
    RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,
                                                                     serializedValue, headers, interceptCallback, remainingWaitMs, true, nowMs);

    if (result.abortForNewBatch) {
      int prevPartition = partition;
      partitioner.onNewBatch(record.topic(), cluster, prevPartition);
      partition = partition(record, serializedKey, serializedValue, cluster);
      tp = new TopicPartition(record.topic(), partition);
      if (log.isTraceEnabled()) {
        log.trace("Retrying append due to new batch creation for topic {} partition {}. The old partition was {}", record.topic(), partition, prevPartition);
      }
      // producer callback will make sure to call both 'callback' and interceptor callback
      interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);

      result = accumulator.append(tp, timestamp, serializedKey,
                                  serializedValue, headers, interceptCallback, remainingWaitMs, false, nowMs);
    }

    if (transactionManager != null && transactionManager.isTransactional())
      transactionManager.maybeAddPartitionToTransaction(tp);
    // æ‰¹æ¬¡å¤§å°å·²ç»æ»¡äº† è·å–æœ‰ä¸€ä¸ªæ–°æ‰¹æ¬¡åˆ›å»º
    if (result.batchIsFull || result.newBatchCreated) {
      log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);
      // å”¤é†’å‘é€çº¿ç¨‹
      this.sender.wakeup();
    }
    return result.future;
    // handling exceptions and record the errors;
    // for API exceptions return them in the future,
    // for other exceptions throw directly
  } catch (ApiException e) {
    log.debug("Exception occurred during message send:", e);
    if (callback != null)
      callback.onCompletion(null, e);
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    return new FutureFailure(e);
  } catch (InterruptedException e) {
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    throw new InterruptException(e);
  } catch (KafkaException e) {
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    throw e;
  } catch (Exception e) {
    // we notify interceptor about all exceptions, since onSend is called before anything else in this method
    this.interceptors.onSendError(record, tp, e);
    throw e;
  }
}
```



### åˆ†åŒºé€‰æ‹©

1ï¼‰å‘é€æ€»ä½“æµç¨‹ä¸­ä»¥ä¸‹ä»£ç ä¸ºåˆ†åŒºé€‰æ‹©ç›¸å…³æµç¨‹ï¼š

```java
// åˆ†åŒºæ“ä½œ
int partition = partition(record, serializedKey, serializedValue, cluster);
tp = new TopicPartition(record.topic(), partition);

// partition()æ–¹æ³•å…·ä½“å¦‚ä¸‹ï¼š
private int partition(ProducerRecord<K, V> record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) {
  Integer partition = record.partition();
  // å¦‚æœæŒ‡å®šåˆ†åŒºï¼ŒæŒ‰ç…§æŒ‡å®šåˆ†åŒºé…ç½®
  return partition != null ?
    partition :
  // åˆ†åŒºå™¨é€‰æ‹©åˆ†åŒº
  partitioner.partition( // ç‚¹å‡» partitionï¼Œè·³è½¬åˆ° Partitioner æ¥å£ï¼Œé€‰æ‹©é»˜è®¤çš„åˆ†åŒºå™¨ DefaultPartitioner --> 2âƒ£ï¸
    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);
}
```



2ï¼‰ç‚¹å‡» partitionï¼Œè·³è½¬åˆ° Partitioner æ¥å£ï¼Œé€‰æ‹©é»˜è®¤çš„åˆ†åŒºå™¨ DefaultPartitioner

```java
public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster,
                     int numPartitions) {
  // æ²¡æœ‰æŒ‡å®škey
  if (keyBytes == null) {
    // æŒ‰ç…§ç²˜æ€§åˆ†åŒºå¤„ç† --> 3âƒ£ï¸
    return stickyPartitionCache.partition(topic, cluster);
  }
  // å¦‚æœæŒ‡å®škey,æŒ‰ç…§keyçš„hashcodeå€¼ å¯¹åˆ†åŒºæ•°æ±‚æ¨¡
  // hash the keyBytes to choose a partition
  return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
}
---------------------------------------------------------
// 3âƒ£ï¸æ²¡æœ‰æŒ‡å®škeyå’Œåˆ†åŒºçš„å¤„ç†æ–¹å¼ stickyPartitionCache.partition(topic, cluster);
public int partition(String topic, Cluster cluster) {
  Integer part = indexCache.get(topic);
  if (part == null) {
    return nextPartition(topic, cluster, -1); // --> 4âƒ£ï¸
  }
  return part;
}
---------------------------------------------------------
// 4âƒ£ï¸nextPartition(topic, cluster, -1);
public int nextPartition(String topic, Cluster cluster, int prevPartition) {
  List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
  Integer oldPart = indexCache.get(topic);
  Integer newPart = oldPart;
  // Check that the current sticky partition for the topic is either not set or that the partition that 
  // triggered the new batch matches the sticky partition that needs to be changed.
  if (oldPart == null || oldPart == prevPartition) {
    List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);
    if (availablePartitions.size() < 1) {
      Integer random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
      newPart = random % partitions.size();
    } else if (availablePartitions.size() == 1) {
      newPart = availablePartitions.get(0).partition();
    } else {
      while (newPart == null || newPart.equals(oldPart)) {
        int random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
        newPart = availablePartitions.get(random % availablePartitions.size()).partition();
      }
    }
    // Only change the sticky partition if it is null or prevPartition matches the current sticky partition.
    if (oldPart == null) {
      indexCache.putIfAbsent(topic, newPart);
    } else {
      indexCache.replace(topic, prevPartition, newPart);
    }
    return indexCache.get(topic);
  }
  return indexCache.get(topic);
}
```



### å‘é€æ¶ˆæ¯å¤§å°æ ¡éªŒ

å‘é€æ€»ä½“æµç¨‹ä¸­çš„ensureValidRecordSize(serializedSize)æ–¹æ³•æ¶‰åŠåˆ°å‘é€æ¶ˆæ¯å¤§å°æ ¡éªŒæ“ä½œ

```java
private void ensureValidRecordSize(int size) {
  // å•æ¡ä¿¡æ¯æœ€å¤§å€¼ maxRequestSize 1m
  if (size > maxRequestSize)
    throw new RecordTooLargeException("The message is " + size +
                                      " bytes when serialized which is larger than " + maxRequestSize + ", which is the value of the " +
                                      ProducerConfig.MAX_REQUEST_SIZE_CONFIG + " configuration.");
  // ç¼“å†²åŒºå†…å­˜æ€»å¤§å°ï¼Œé»˜è®¤32m
  if (size > totalMemorySize)
    throw new RecordTooLargeException("The message is " + size +
                                      " bytes when serialized which is larger than the total memory buffer you have configured with the " +
                                      ProducerConfig.BUFFER_MEMORY_CONFIG +
                                      " configuration.");
}
```





### å†…å­˜/ç¼“å­˜æ± 

å‘é€æ€»ä½“æµç¨‹ä¸­çš„`RecordAccumulator.RecordAppendResult result = accumulator.append(...);`

```java
public RecordAppendResult append(TopicPartition tp,
                                 long timestamp,
                                 byte[] key,
                                 byte[] value,
                                 Header[] headers,
                                 Callback callback,
                                 long maxTimeToBlock,
                                 boolean abortOnNewBatch,
                                 long nowMs) throws InterruptedException {
  // We keep track of the number of appending thread to make sure we do not miss batches in
  // abortIncompleteBatches().
  appendsInProgress.incrementAndGet();
  ByteBuffer buffer = null;
  if (headers == null) headers = Record.EMPTY_HEADERS;
  try {
    // check if we have an in-progress batch
    // è·å–æˆ–è€…åˆ›å»ºä¸€ä¸ªé˜Ÿåˆ—ï¼ˆæŒ‰ç…§æ¯ä¸ªä¸»é¢˜çš„åˆ†åŒºï¼‰
    Deque<ProducerBatch> dq = getOrCreateDeque(tp);
    synchronized (dq) {
      if (closed)
        throw new KafkaException("Producer closed while send in progress");
      // å°è¯•å‘é˜Ÿåˆ—é‡Œé¢æ·»åŠ æ•°æ®(æ²¡æœ‰åˆ†é…å†…å­˜ã€æ‰¹æ¬¡å¯¹è±¡ï¼Œæ‰€ä»¥å¤±è´¥)
      RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);
      if (appendResult != null)
        return appendResult;
    }

    // we don't have an in-progress record batch try to allocate a new batch
    if (abortOnNewBatch) {
      // Return a result that will cause another call to append.
      return new RecordAppendResult(null, false, false, true);
    }

    byte maxUsableMagic = apiVersions.maxUsableProduceMagic();
    // this.batchSize é»˜è®¤16k    æ•°æ®å¤§å°17k
    int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));
    log.trace("Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms", size, tp.topic(), tp.partition(), maxTimeToBlock);
    // ç”³è¯·å†…å­˜  å†…å­˜æ± æ ¹æ®æ‰¹æ¬¡å¤§å°(é»˜è®¤16k)å’Œæ¶ˆæ¯å¤§å°ä¸­æœ€å¤§å€¼ï¼Œåˆ†é…å†…å­˜   ğŸ˜åŒç«¯é˜Ÿåˆ— 
    buffer = free.allocate(size, maxTimeToBlock);

    // Update the current time in case the buffer allocation blocked above.
    nowMs = time.milliseconds();
    synchronized (dq) {
      // Need to check if producer is closed again after grabbing the dequeue lock.
      if (closed)
        throw new KafkaException("Producer closed while send in progress");
      // å°è¯•å‘é˜Ÿåˆ—é‡Œé¢æ·»åŠ æ•°æ®(æœ‰å†…å­˜ï¼Œä½†æ˜¯æ²¡æœ‰æ‰¹æ¬¡å¯¹è±¡)
      RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);
      if (appendResult != null) {
        // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often...
        return appendResult;
      }
      // å°è£…å†…å­˜buffer
      MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);
      // æ ¹æ®å†…å­˜å¤§å°å°è£…æ‰¹æ¬¡(æœ‰å†…å­˜ã€æœ‰æ‰¹æ¬¡å¯¹è±¡)
      ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, nowMs);
      // å°è¯•å‘é˜Ÿåˆ—é‡Œé¢æ·»åŠ æ•°æ®
      FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers,
                                                                           callback, nowMs));
      // æŠŠæ–°åˆ›å»ºçš„æ‰¹æ¬¡æ”¾åˆ°é˜Ÿåˆ—æœ«å°¾
      dq.addLast(batch);
      incomplete.add(batch);

      // Don't deallocate this buffer in the finally block as it's being used in the record batch
      buffer = null;
      return new RecordAppendResult(future, dq.size() > 1 || batch.isFull(), true, false);
    }
  } finally {
    // å¦‚æœå‘ç”Ÿå¼‚å¸¸ï¼Œé‡Šæ”¾å†…å­˜
    if (buffer != null)
      free.deallocate(buffer);
    appendsInProgress.decrementAndGet();
  }
}
```



## **sender** çº¿ç¨‹å‘é€æ•°æ®

![image-20220512000146708](/assets/imgs/image-20220512000146708.png)



1ï¼‰è¿›å…¥ sender å‘é€çº¿ç¨‹çš„ `run()`æ–¹æ³•ã€‚

```java
public void run() {
  log.debug("Starting Kafka producer I/O thread.");

  // main loop, runs until close is called
  while (running) {
    try {
      // sender çº¿ç¨‹ä»ç¼“å†²åŒºå‡†å¤‡æ‹‰å–æ•°æ®ï¼Œåˆšå¯åŠ¨æ‹‰ä¸åˆ°æ•°æ® --> 2âƒ£ï¸
      runOnce();
    } catch (Exception e) {
      log.error("Uncaught error in kafka producer I/O thread: ", e);
    }
  }
  ...
}
```



2ï¼‰`runOnce()`æ–¹æ³•

```java
void runOnce() {
  // å¦‚æœæ˜¯äº‹åŠ¡æ“ä½œï¼ŒæŒ‰ç…§å¦‚ä¸‹å¤„ç†
  if (transactionManager != null) {
    try {
      transactionManager.maybeResolveSequences();

      // do not continue sending if the transaction manager is in a failed state
      if (transactionManager.hasFatalError()) {
        RuntimeException lastError = transactionManager.lastError();
        if (lastError != null)
          maybeAbortBatches(lastError);
        // è·å–æœåŠ¡å™¨ç«¯å“åº” -->7âƒ£ï¸
        client.poll(retryBackoffMs, time.milliseconds()); 
        return;
      }

      // Check whether we need a new producerId. If so, we will enqueue an InitProducerId
      // request which will be sent below
      transactionManager.bumpIdempotentEpochAndResetIdIfNeeded();

      if (maybeSendAndPollTransactionalRequest()) {
        return;
      }
    } catch (AuthenticationException e) {
      // This is already logged as error, but propagated here to perform any clean ups.
      log.trace("Authentication exception while processing transactional request", e);
      transactionManager.authenticationFailed(e);
    }
  }

  long currentTimeMs = time.milliseconds();
  // å°†å‡†å¤‡å¥½çš„æ•°æ®å‘é€åˆ°æœåŠ¡å™¨ç«¯ --> 3âƒ£ï¸
  long pollTimeout = sendProducerData(currentTimeMs);
  // è·å–å‘é€ç»“æœ
  client.poll(pollTimeout, currentTimeMs);
}
```



3ï¼‰è·å–è¦å‘é€æ•°æ®çš„ç»†èŠ‚ `sendProducerData()`

```java
private long sendProducerData(long now) {
  // è·å–å…ƒæ•°æ®
  Cluster cluster = metadata.fetch();
  // get the list of partitions with data ready to send
  // åˆ¤æ–­32mç¼“å­˜æ˜¯å¦å‡†å¤‡å¥½(linger.ms) --> 4âƒ£ï¸
  RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);

  // å¦‚æœLeaderä¿¡æ¯ä¸çŸ¥é“ï¼Œæ˜¯ä¸èƒ½å‘é€æ•°æ®çš„
  // if there are any partitions whose leaders are not known yet, force metadata update
  if (!result.unknownLeaderTopics.isEmpty()) {
    // The set of topics with unknown leader contains topics with leader election pending as well as
    // topics which may have expired. Add the topic again to metadata to ensure it is included
    // and request metadata update, since there are messages to send to the topic.
    for (String topic : result.unknownLeaderTopics)
      this.metadata.add(topic, now);

    log.debug("Requesting metadata update due to unknown leader topics from the batched records: {}",
              result.unknownLeaderTopics);
    this.metadata.requestUpdate();
  }

  // remove any nodes we aren't ready to send to
  // åˆ é™¤æ‰æ²¡æœ‰å‡†å¤‡å¥½å‘é€çš„æ•°æ®
  Iterator<Node> iter = result.readyNodes.iterator();
  long notReadyTimeout = Long.MAX_VALUE;
  while (iter.hasNext()) {
    Node node = iter.next();
    if (!this.client.ready(node, now)) {
      iter.remove();
      notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));
    }
  }

  // create produce requests
  // å‘å¾€åŒä¸€ä¸ª broker èŠ‚ç‚¹çš„æ•°æ®ï¼Œæ‰“åŒ…ä¸ºä¸€ä¸ªè¯·æ±‚æ‰¹æ¬¡ --> 5âƒ£ï¸
  Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);
  addToInflightBatches(batches);
  if (guaranteeMessageOrder) {
    // Mute all the partitions drained
    for (List<ProducerBatch> batchList : batches.values()) {
      for (ProducerBatch batch : batchList)
        this.accumulator.mutePartition(batch.topicPartition);
    }
  }

  accumulator.resetNextBatchExpiryTime();
  List<ProducerBatch> expiredInflightBatches = getExpiredInflightBatches(now);
  List<ProducerBatch> expiredBatches = this.accumulator.expiredBatches(now);
  expiredBatches.addAll(expiredInflightBatches);

  // Reset the producer id if an expired batch has previously been sent to the broker. Also update the metrics
  // for expired batches. see the documentation of @TransactionState.resetIdempotentProducerId to understand why
  // we need to reset the producer id here.
  if (!expiredBatches.isEmpty())
    log.trace("Expired {} batches in accumulator", expiredBatches.size());
  for (ProducerBatch expiredBatch : expiredBatches) {
    String errorMessage = "Expiring " + expiredBatch.recordCount + " record(s) for " + expiredBatch.topicPartition
      + ":" + (now - expiredBatch.createdMs) + " ms has passed since batch creation";
    failBatch(expiredBatch, new TimeoutException(errorMessage), false);
    if (transactionManager != null && expiredBatch.inRetry()) {
      // This ensures that no new batches are drained until the current in flight batches are fully resolved.
      transactionManager.markSequenceUnresolved(expiredBatch);
    }
  }
  sensors.updateProduceRequestMetrics(batches);

  // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately
  // loop and try sending more data. Otherwise, the timeout will be the smaller value between next batch expiry
  // time, and the delay time for checking data availability. Note that the nodes may have data that isn't yet
  // sendable due to lingering, backing off, etc. This specifically does not include nodes with sendable data
  // that aren't ready to send since they would cause busy looping.
  long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);
  pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now);
  pollTimeout = Math.max(pollTimeout, 0);
  if (!result.readyNodes.isEmpty()) {
    log.trace("Nodes with data ready to send: {}", result.readyNodes);
    // if some partitions are already ready to be sent, the select time would be 0;
    // otherwise if some partition already has some data accumulated but not ready yet,
    // the select time will be the time difference between now and its linger expiry time;
    // otherwise the select time will be the time difference between now and the metadata expiry time;
    pollTimeout = 0;
  }
  // å‘é€è¯·æ±‚ --> 6âƒ£ï¸
  sendProduceRequests(batches, now);
  return pollTimeout;
}
```



4ï¼‰åˆ¤æ–­32mç¼“å­˜æ˜¯å¦å‡†å¤‡å¥½

```java
public ReadyCheckResult ready(Cluster cluster, long nowMs) {
  Set<Node> readyNodes = new HashSet<>();
  long nextReadyCheckDelayMs = Long.MAX_VALUE;
  Set<String> unknownLeaderTopics = new HashSet<>();

  boolean exhausted = this.free.queued() > 0;
  for (Map.Entry<TopicPartition, Deque<ProducerBatch>> entry : this.batches.entrySet()) {
    Deque<ProducerBatch> deque = entry.getValue();
    synchronized (deque) {
      // When producing to a large number of partitions, this path is hot and deques are often empty.
      // We check whether a batch exists first to avoid the more expensive checks whenever possible.
      ProducerBatch batch = deque.peekFirst();
      if (batch != null) {
        TopicPartition part = entry.getKey();
        Node leader = cluster.leaderFor(part);
        if (leader == null) {
          // This is a partition for which leader is not known, but messages are available to send.
          // Note that entries are currently not removed from batches when deque is empty.
          unknownLeaderTopics.add(part.topic());
        } else if (!readyNodes.contains(leader) && !isMuted(part)) {
          long waitedTimeMs = batch.waitedTimeMs(nowMs);
          // å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡æ‹‰å–ï¼Œ  ä¸”ç­‰å¾…æ—¶é—´å°äºé‡è¯•æ—¶é—´ é»˜è®¤100ms ,backingOff=true
          boolean backingOff = batch.attempts() > 0 && waitedTimeMs < retryBackoffMs;
          // å¦‚æœbackingOffæ˜¯true å–retryBackoffMsï¼› å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡æ‹‰å–å–lingerMsï¼Œé»˜è®¤0
          long timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;
          // æ‰¹æ¬¡å¤§å°æ»¡è¶³å‘é€æ¡ä»¶
          boolean full = deque.size() > 1 || batch.isFull();
          // å¦‚æœç­‰å¾…çš„æ—¶é—´è¶…è¿‡äº† timeToWaitMsï¼Œexpired=trueï¼Œè¡¨ç¤ºå¯ä»¥å‘é€æ•°æ®
          boolean expired = waitedTimeMs >= timeToWaitMs;
          boolean transactionCompleting = transactionManager != null && transactionManager.isCompleting();
          // full linger.ms
          boolean sendable = full
            || expired
            || exhausted
            || closed
            || flushInProgress()
            || transactionCompleting;
          if (sendable && !backingOff) {
            readyNodes.add(leader);
          } else {
            long timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, 0);
            // Note that this results in a conservative estimate since an un-sendable partition may have
            // a leader that will later be found to have sendable data. However, this is good enough
            // since we'll just wake up and then sleep again for the remaining time.
            nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);
          }
        }
      }
    }
  }
  return new ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);
}
```



5ï¼‰å‘å¾€åŒä¸€ä¸ª **broker** èŠ‚ç‚¹çš„æ•°æ®ï¼Œæ‰“åŒ…ä¸ºä¸€ä¸ªè¯·æ±‚æ‰¹æ¬¡ã€‚

```java
public Map<Integer, List<ProducerBatch>> drain(Cluster cluster, Set<Node> nodes, int maxSize, long now) {
  if (nodes.isEmpty())
    return Collections.emptyMap();

  Map<Integer, List<ProducerBatch>> batches = new HashMap<>();
  for (Node node : nodes) {
    List<ProducerBatch> ready = drainBatchesForOneNode(cluster, node, maxSize, now);
    batches.put(node.id(), ready);
  }
  return batches;
}
```



6ï¼‰å‘é€è¯·æ±‚

```java
private void sendProduceRequest(long now, int destination, short acks, int timeout, List<ProducerBatch> batches) {
  if (batches.isEmpty())
    return;

  final Map<TopicPartition, ProducerBatch> recordsByPartition = new HashMap<>(batches.size());

  // find the minimum magic version used when creating the record sets
  byte minUsedMagic = apiVersions.maxUsableProduceMagic();
  for (ProducerBatch batch : batches) {
    if (batch.magic() < minUsedMagic)
      minUsedMagic = batch.magic();
  }
  ProduceRequestData.TopicProduceDataCollection tpd = new ProduceRequestData.TopicProduceDataCollection();
  for (ProducerBatch batch : batches) {
    TopicPartition tp = batch.topicPartition;
    MemoryRecords records = batch.records();

    // down convert if necessary to the minimum magic used. In general, there can be a delay between the time
    // that the producer starts building the batch and the time that we send the request, and we may have
    // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use
    // the new message format, but found that the broker didn't support it, so we need to down-convert on the
    // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may
    // not all support the same message format version. For example, if a partition migrates from a broker
    // which is supporting the new magic version to one which doesn't, then we will need to convert.
    if (!records.hasMatchingMagic(minUsedMagic))
      records = batch.records().downConvert(minUsedMagic, 0, time).records();
    ProduceRequestData.TopicProduceData tpData = tpd.find(tp.topic());
    if (tpData == null) {
      tpData = new ProduceRequestData.TopicProduceData().setName(tp.topic());
      tpd.add(tpData);
    }
    tpData.partitionData().add(new ProduceRequestData.PartitionProduceData()
                               .setIndex(tp.partition())
                               .setRecords(records));
    recordsByPartition.put(tp, batch);
  }

  String transactionalId = null;
  if (transactionManager != null && transactionManager.isTransactional()) {
    transactionalId = transactionManager.transactionalId();
  }

  ProduceRequest.Builder requestBuilder = ProduceRequest.forMagic(minUsedMagic,
                                                                  new ProduceRequestData()
                                                                  .setAcks(acks)
                                                                  .setTimeoutMs(timeout)
                                                                  .setTransactionalId(transactionalId)
                                                                  .setTopicData(tpd));
  RequestCompletionHandler callback = response -> handleProduceResponse(response, recordsByPartition, time.milliseconds());

  String nodeId = Integer.toString(destination);
  // åˆ›å»ºå‘é€è¯·æ±‚å¯¹è±¡
  ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0,
                                                        requestTimeoutMs, callback);
  // å‘é€è¯·æ±‚
  client.send(clientRequest, now);
  log.trace("Sent produce request to {}: {}", nodeId, requestBuilder);
}
---------------------------------------------------------
public void send(ClientRequest request, long now) {
  doSend(request, false, now);
}
---------------------------------------------------------
private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long now, AbstractRequest request) {
  String destination = clientRequest.destination();
  RequestHeader header = clientRequest.makeHeader(request.version());
  if (log.isDebugEnabled()) {
    log.debug("Sending {} request with header {} and timeout {} to node {}: {}",
              clientRequest.apiKey(), header, clientRequest.requestTimeoutMs(), destination, request);
  }
  Send send = request.toSend(header);
  InFlightRequest inFlightRequest = new InFlightRequest(
    clientRequest,
    header,
    isInternalRequest,
    request,
    send,
    now);
  // æ·»åŠ è¯·æ±‚åˆ°inflint
  this.inFlightRequests.add(inFlightRequest);
  // å‘é€æ•°æ®
  selector.send(new NetworkSend(clientRequest.destination(), send));
}
```



7ï¼‰è·å–æœåŠ¡å™¨ç«¯å“åº”

```java
public List<ClientResponse> poll(long timeout, long now) {
  ensureActive();

  if (!abortedSends.isEmpty()) {
    // If there are aborted sends because of unsupported version exceptions or disconnects,
    // handle them immediately without waiting for Selector#poll.
    List<ClientResponse> responses = new ArrayList<>();
    handleAbortedSends(responses);
    completeResponses(responses);
    return responses;
  }

  long metadataTimeout = metadataUpdater.maybeUpdate(now);
  try {
    this.selector.poll(Utils.min(timeout, metadataTimeout, defaultRequestTimeoutMs));
  } catch (IOException e) {
    log.error("Unexpected error during I/O", e);
  }

  // process completed actions
  // è·å–å‘é€åçš„å“åº”
  long updatedNow = this.time.milliseconds();
  List<ClientResponse> responses = new ArrayList<>();
  handleCompletedSends(responses, updatedNow);
  handleCompletedReceives(responses, updatedNow);
  handleDisconnections(responses, updatedNow);
  handleConnections();
  handleInitiateApiVersionRequests(updatedNow);
  handleTimedOutConnections(responses, updatedNow);
  handleTimedOutRequests(responses, updatedNow);
  completeResponses(responses);

  return responses;
}
```







# æ¶ˆè´¹è€…æºç 

1ï¼‰æ¶ˆè´¹è€…ç»„åˆå§‹åŒ–æµç¨‹

coordinatorï¼šè¾…åŠ©å®ç°æ¶ˆè´¹è€…ç»„çš„åˆå§‹åŒ–å’Œåˆ†åŒºçš„åˆ†é…ã€‚

- coordinatorèŠ‚ç‚¹é€‰æ‹©=groupidçš„nashcodeå€¼%50(\_consumer_offsetsçš„åˆ†åŒºæ•°é‡)_
- ä¾‹å¦‚ï¼šgroupidçš„hashcodefå€¼=1ï¼Œ1%50=1ï¼Œé‚£ä¹ˆ\_consumer_offsetsä¸»é¢˜çš„1å·åˆ†åŒºï¼Œåœ¨å“ªä¸ªbrokerä¸Šï¼Œ**å°±é€‰æ‹©è¿™ä¸ªèŠ‚ç‚¹çš„coordinator ä½œä¸ºè¿™ä¸ªæ¶ˆè´¹è€…ç»„çš„è€å¤§ã€‚æ¶ˆè´¹è€…ç»„ä¸‹çš„æ‰€æœ‰çš„æ¶ˆè´¹è€…æäº¤offsetçš„æ—¶å€™å°±å¾€è¿™ä¸ªåˆ†åŒºå»æäº¤offsetã€‚**

![image-20220506165927636](/assets/imgs/image-20220506165927636.png)

âš ï¸è§¦å‘å†å¹³è¡¡çš„ä¸¤ä¸ªæ¡ä»¶ï¼š

- æ¯ä¸ªæ¶ˆè´¹è€…éƒ½ä¼šå’Œcoordinatorä¿æŒå¿ƒè·³(é»˜è®¤3s)ï¼Œä¸€æ—¦è¶…æ—¶ (session.timeout.ms=45s)ï¼Œè¯¥æ¶ˆè´¹è€…ä¼šè¢«ç§»é™¤ï¼Œå¹¶è§¦å‘å†å¹³è¡¡;
- æˆ–è€…æ¶ˆè´¹è€…å¤„ç†æ¶ˆæ¯çš„æ—¶é—´è¿‡é•¿(è¶…è¿‡max.poll.interval.ms5åˆ†é’Ÿ)ï¼Œä¹Ÿä¼šè§¦å‘å†å¹³è¡¡



2ï¼‰æ¶ˆè´¹æµç¨‹

1. åˆ›å»ºCOnsumerNetworkClientï¼Œæ¥æ”¶æ¶ˆè´¹è€…çš„æ¶ˆè´¹è¯·æ±‚sendFetches
2. ConsumerNetworkClienté€šè¿‡sendæ–¹æ³•æ‹‰å»brokerä¸­çš„æ¶ˆæ¯ï¼Œbrokerä¹Ÿæœ‰ç›¸åº”onSuccesså›è°ƒæ–¹æ³•
3. æŠŠæ‹‰å»çš„æ¶ˆæ¯æ”¾å…¥completedFetchesé˜Ÿåˆ—ï¼Œæ¶ˆè´¹è€…ä¼šæŒ‰æ‰¹æ¬¡æ‹‰å–æ•°æ®
4. ååºåˆ—åŒ–/æ‹¦æˆªå™¨/å¤„ç†æ•°æ®

![image-20220506180242291](/assets/imgs/image-20220506180242291.png)



## æ¶ˆè´¹è€…åˆå§‹åŒ–

![image-20220512213120720](/assets/imgs/image-20220512213120720.png)

1ï¼‰ æŸ¥çœ‹`KafkaConsumer `æ„é€ æ–¹æ³•

```java
KafkaConsumer(ConsumerConfig config, Deserializer<K> keyDeserializer, Deserializer<V> valueDeserializer) {
  try {
    // æ¶ˆè´¹ç»„å¹³è¡¡
    GroupRebalanceConfig groupRebalanceConfig = new GroupRebalanceConfig(config,
                                                                         GroupRebalanceConfig.ProtocolType.CONSUMER);
    // è·å–æ¶ˆè´¹è€…ç»„id
    this.groupId = Optional.ofNullable(groupRebalanceConfig.groupId);
    // å®¢æˆ·ç«¯id
    this.clientId = config.getString(CommonClientConfigs.CLIENT_ID_CONFIG);

    LogContext logContext;

    // If group.instance.id is set, we will append it to the log context.
    if (groupRebalanceConfig.groupInstanceId.isPresent()) {
      logContext = new LogContext("[Consumer instanceId=" + groupRebalanceConfig.groupInstanceId.get() +
                                  ", clientId=" + clientId + ", groupId=" + groupId.orElse("null") + "] ");
    } else {
      logContext = new LogContext("[Consumer clientId=" + clientId + ", groupId=" + groupId.orElse("null") + "] ");
    }

    this.log = logContext.logger(getClass());
    boolean enableAutoCommit = config.maybeOverrideEnableAutoCommit();
    groupId.ifPresent(groupIdStr -> {
      if (groupIdStr.isEmpty()) {
        log.warn("Support for using the empty group id by consumers is deprecated and will be removed in the next major release.");
      }
    });

    log.debug("Initializing the Kafka consumer");
    // å®¢æˆ·ç«¯è¯·æ±‚æœåŠ¡ç«¯ç­‰å¾…æ—¶é—´request.timeout.ms é»˜è®¤æ˜¯30s
    this.requestTimeoutMs = config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG);
    this.defaultApiTimeoutMs = config.getInt(ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);
    this.time = Time.SYSTEM;
    this.metrics = buildMetrics(config, time, clientId);
    // é‡è¯•æ—¶é—´ 100
    this.retryBackoffMs = config.getLong(ConsumerConfig.RETRY_BACKOFF_MS_CONFIG);

    // æ‹¦æˆªå™¨é…ç½®
    List<ConsumerInterceptor<K, V>> interceptorList = (List) config.getConfiguredInstances(
      ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG,
      ConsumerInterceptor.class,
      Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId));
    this.interceptors = new ConsumerInterceptors<>(interceptorList);
    // keyå’Œvalue çš„ååºåˆ—åŒ–
    if (keyDeserializer == null) {
      this.keyDeserializer = config.getConfiguredInstance(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, Deserializer.class);
      this.keyDeserializer.configure(config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId)), true);
    } else {
      config.ignore(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);
      this.keyDeserializer = keyDeserializer;
    }
    if (valueDeserializer == null) {
      this.valueDeserializer = config.getConfiguredInstance(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, Deserializer.class);
      this.valueDeserializer.configure(config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId)), false);
    } else {
      config.ignore(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);
      this.valueDeserializer = valueDeserializer;
    }
    // offsetä»ä»€ä¹ˆä½ç½®å¼€å§‹æ¶ˆè´¹ é»˜è®¤ï¼Œlatest
    OffsetResetStrategy offsetResetStrategy = OffsetResetStrategy.valueOf(config.getString(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).toUpperCase(Locale.ROOT));
    this.subscriptions = new SubscriptionState(logContext, offsetResetStrategy);
    ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keyDeserializer,
                                                                                          valueDeserializer, metrics.reporters(), interceptorList);
    // å…ƒæ•°æ®
    // retryBackoffMs é‡è¯•æ—¶é—´
    // æ˜¯å¦å…è®¸è®¿é—®ç³»ç»Ÿä¸»é¢˜ exclude.internal.topics  é»˜è®¤æ˜¯trueï¼Œè¡¨ç¤ºä¸å…è®¸
    // æ˜¯å¦å…è®¸è‡ªåŠ¨åˆ›å»ºtopic  allow.auto.create.topics é»˜è®¤æ˜¯true
    this.metadata = new ConsumerMetadata(retryBackoffMs,
                                         config.getLong(ConsumerConfig.METADATA_MAX_AGE_CONFIG),
                                         !config.getBoolean(ConsumerConfig.EXCLUDE_INTERNAL_TOPICS_CONFIG),
                                         config.getBoolean(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG),
                                         subscriptions, logContext, clusterResourceListeners);
    // è¿æ¥kafkaé›†ç¾¤
    List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
      config.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG), config.getString(ConsumerConfig.CLIENT_DNS_LOOKUP_CONFIG));
    this.metadata.bootstrap(addresses);
    String metricGrpPrefix = "consumer";

    FetcherMetricsRegistry metricsRegistry = new FetcherMetricsRegistry(Collections.singleton(CLIENT_ID_METRIC_TAG), metricGrpPrefix);
    ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config, time, logContext);
    this.isolationLevel = IsolationLevel.valueOf(
      config.getString(ConsumerConfig.ISOLATION_LEVEL_CONFIG).toUpperCase(Locale.ROOT));
    Sensor throttleTimeSensor = Fetcher.throttleTimeSensor(metrics, metricsRegistry);
    // å¿ƒè·³æ—¶é—´,é»˜è®¤3s
    int heartbeatIntervalMs = config.getInt(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG);

    ApiVersions apiVersions = new ApiVersions();
    // åˆ›å»ºå®¢æˆ·ç«¯å¯¹è±¡
    // è¿æ¥é‡è¯•æ—¶é—´ é»˜è®¤50ms
    // æœ€å¤§è¿æ¥é‡è¯•æ—¶é—´ é»˜è®¤1s
    // å‘é€ç¼“å­˜ é»˜è®¤128kb
    // æ¥æ”¶ç¼“å­˜  é»˜è®¤64kb
    // å®¢æˆ·ç«¯è¯·æ±‚æœåŠ¡ç«¯ç­‰å¾…æ—¶é—´request.timeout.ms é»˜è®¤æ˜¯30s
    NetworkClient netClient = new NetworkClient(
      new Selector(config.getLong(ConsumerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), metrics, time, metricGrpPrefix, channelBuilder, logContext),
      this.metadata,
      clientId,
      100, // a fixed large enough value will suffice for max in-flight requests
      config.getLong(ConsumerConfig.RECONNECT_BACKOFF_MS_CONFIG),
      config.getLong(ConsumerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),
      config.getInt(ConsumerConfig.SEND_BUFFER_CONFIG),
      config.getInt(ConsumerConfig.RECEIVE_BUFFER_CONFIG),
      config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG),
      config.getLong(ConsumerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),
      config.getLong(ConsumerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG),
      time,
      true,
      apiVersions,
      throttleTimeSensor,
      logContext);
    // æ¶ˆè´¹è€…å®¢æˆ·ç«¯
    // å®¢æˆ·ç«¯è¯·æ±‚æœåŠ¡ç«¯ç­‰å¾…æ—¶é—´request.timeout.ms é»˜è®¤æ˜¯30s
    this.client = new ConsumerNetworkClient(
      logContext,
      netClient,
      metadata,
      time,
      retryBackoffMs,
      config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG),
      heartbeatIntervalMs); //Will avoid blocking an extended period of time to prevent heartbeat thread starvation
    // è·å–æ¶ˆè´¹è€…åˆ†åŒºåˆ†é…ç­–ç•¥
    this.assignors = ConsumerPartitionAssignor.getAssignorInstances(
      config.getList(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG),
      config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId))
    );

    // no coordinator will be constructed for the default (null) group id
    // åˆ›å»ºæ¶ˆè´¹è€…åè°ƒå™¨
    // auto.commit.interval.ms  è‡ªåŠ¨æäº¤offsetæ—¶é—´ é»˜è®¤5s
    this.coordinator = !groupId.isPresent() ? null :
    new ConsumerCoordinator(groupRebalanceConfig,
                            logContext,
                            this.client,
                            assignors,
                            this.metadata,
                            this.subscriptions,
                            metrics,
                            metricGrpPrefix,
                            this.time,
                            enableAutoCommit,
                            config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG),
                            this.interceptors,
                            config.getBoolean(ConsumerConfig.THROW_ON_FETCH_STABLE_OFFSET_UNSUPPORTED));
    // é…ç½®æŠ“æ•°æ®çš„å‚æ•°
    // fetch.min.bytes é»˜è®¤æœ€å°‘ä¸€æ¬¡æŠ“å–1ä¸ªå­—èŠ‚
    // fetch.max.bytes é»˜è®¤æœ€å¤šä¸€æ¬¡æŠ“å–50m
    // fetch.max.wait.ms æŠ“å–ç­‰å¾…æœ€å¤§æ—¶é—´ 500ms
    // max.partition.fetch.bytes é»˜è®¤æ˜¯1m
    // max.poll.records  é»˜è®¤ä¸€æ¬¡å¤„ç†500æ¡
    // key å’Œ value çš„ååºåˆ—åŒ–
    this.fetcher = new Fetcher<>(
      logContext,
      this.client,
      config.getInt(ConsumerConfig.FETCH_MIN_BYTES_CONFIG),
      config.getInt(ConsumerConfig.FETCH_MAX_BYTES_CONFIG),
      config.getInt(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG),
      config.getInt(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG),
      config.getInt(ConsumerConfig.MAX_POLL_RECORDS_CONFIG),
      config.getBoolean(ConsumerConfig.CHECK_CRCS_CONFIG),
      config.getString(ConsumerConfig.CLIENT_RACK_CONFIG),
      this.keyDeserializer,
      this.valueDeserializer,
      this.metadata,
      this.subscriptions,
      metrics,
      metricsRegistry,
      this.time,
      this.retryBackoffMs,
      this.requestTimeoutMs,
      isolationLevel,
      apiVersions);

    this.kafkaConsumerMetrics = new KafkaConsumerMetrics(metrics, metricGrpPrefix);

    config.logUnused();
    AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());
    log.debug("Kafka consumer initialized");
  } catch (Throwable t) {
    // call close methods if internal objects are already constructed; this is to prevent resource leak. see KAFKA-2121
    // we do not need to call `close` at all when `log` is null, which means no internal objects were initialized.
    if (this.log != null) {
      close(0, true);
    }
    // now propagate the exception
    throw new KafkaException("Failed to construct kafka consumer", t);
  }
}
```



## æ¶ˆè´¹è€…è®¢é˜…ä¸»é¢˜

![image-20220512213158671](/assets/imgs/image-20220512213158671.png)

1ï¼‰`KafkaConsumer.subscribe(Collection<String> topics, ConsumerRebalanceListener listener)`

```java
public void subscribe(Collection<String> topics, ConsumerRebalanceListener listener) {
  acquireAndEnsureOpen();
  try {
    maybeThrowInvalidGroupIdException();
    // è¦è®¢é˜…çš„ä¸»é¢˜å¦‚æœä¸ºnull ï¼Œç›´æ¥æŠ›å¼‚å¸¸
    if (topics == null)
      throw new IllegalArgumentException("Topic collection to subscribe to cannot be null");
    // è¦è®¢é˜…çš„ä¸»é¢˜å¦‚æœä¸ºç©º
    if (topics.isEmpty()) {
      // treat subscribing to empty topic list as the same as unsubscribing
      this.unsubscribe();
    } else {
      // æ­£å¸¸çš„å¤„ç†æ“ä½œ
      for (String topic : topics) {
        // å¦‚æœä¸ºç©º  æŠ›å¼‚å¸¸
        if (Utils.isBlank(topic))
          throw new IllegalArgumentException("Topic collection to subscribe to cannot contain null or empty topic");
      }

      throwIfNoAssignorsConfigured();
      // æ¸…ç©ºè®¢é˜…å¼‚å¸¸ä¸»é¢˜çš„ç¼“å­˜æ•°æ®
      fetcher.clearBufferedDataForUnassignedTopics(topics);
      log.info("Subscribed to topic(s): {}", Utils.join(topics, ", "));
      // åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ”¹è®¢é˜…ä¸»é¢˜ï¼Œå¦‚æœéœ€è¦æ›´æ”¹ä¸»é¢˜ï¼Œåˆ™æ›´æ–°å…ƒæ•°æ®ä¿¡æ¯ï¼ˆä¸»é¢˜äº†ä¸€ä¸ªç›‘å¬å™¨listenerï¼‰ --> 2âƒ£ï¸
      if (this.subscriptions.subscribe(new HashSet<>(topics), listener))
        // æ›´æ–°è®¢é˜…ä¿¡æ¯ --> 3âƒ£ï¸
        metadata.requestUpdateForNewTopics();
    }
  } finally {
    release();
  }
}
```



2ï¼‰`SubscriptionState.subscribe(Set<String> topics, ConsumerRebalanceListener listener)`

```java
public synchronized boolean subscribe(Set<String> topics, ConsumerRebalanceListener listener) {
  // æ³¨å†Œè´Ÿè½½å‡è¡¡ç›‘å¬å™¨
  registerRebalanceListener(listener);
  // æŒ‰ç…§ä¸»é¢˜è‡ªåŠ¨è®¢é˜…æ¨¡å¼
  setSubscriptionType(SubscriptionType.AUTO_TOPICS);
  // åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ”¹è®¢é˜…çš„ä¸»é¢˜
  return changeSubscription(topics);
}
---------------------------------------------------------
private boolean changeSubscription(Set<String> topicsToSubscribe) {
  // å¦‚æœä¼ å…¥çš„topics å’Œä»¥å‰è®¢é˜…çš„ä¸»é¢˜ä¸€è‡´ï¼Œé‚£å°±ä¸éœ€è¦æ›´æ”¹å¯¹åº”è®¢é˜…çš„ä¸»é¢˜
  if (subscription.equals(topicsToSubscribe))
    return false;

  subscription = topicsToSubscribe;
  return true;
}
```



3ï¼‰å¦‚æœè®¢é˜…çš„å’Œä»¥å‰ä¸ä¸€è‡´ï¼Œéœ€è¦æ›´æ–°å…ƒæ•°æ®ä¿¡æ¯

```java
public synchronized int requestUpdateForNewTopics() {
  // Override the timestamp of last refresh to let immediate update.
  this.lastRefreshMs = 0;
  this.needPartialUpdate = true;
  this.requestVersion++;
  return this.updateVersion;
}
```



## æ¶ˆè´¹è€…æ‹‰å–å’Œå¤„ç†æ•°æ®

![image-20220512213225174](/assets/imgs/image-20220512213225174.png)

1ï¼‰`KafkaConsumer.poll()`

```java
public ConsumerRecords<K, V> poll(final Duration timeout) {
  return poll(time.timer(timeout), true);
}

private ConsumerRecords<K, V> poll(final Timer timer, final boolean includeMetadataInTimeout) {
  acquireAndEnsureOpen();
  try {
    // è®°å½•å¼€å§‹æ‹‰å–æ¶ˆæ¯æ—¶é—´
    this.kafkaConsumerMetrics.recordPollStart(timer.currentTimeMs());

    if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) {
      throw new IllegalStateException("Consumer is not subscribed to any topics or assigned any partitions");
    }

    do {
      client.maybeTriggerWakeup();

      if (includeMetadataInTimeout) {
        // æ¶ˆè´¹è€…æˆ–è€…æ¶ˆè´¹è€…ç»„çš„åˆå§‹åŒ– --> 2âƒ£ï¸
        // try to update assignment metadata BUT do not need to block on the timer for join group
        updateAssignmentMetadataIfNeeded(timer, false);
      } else {
        while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE), true)) {
          log.warn("Still waiting for metadata");
        }
      }
      // æŠ“å–æ•°æ® --> 3âƒ£ï¸
      final Map<TopicPartition, List<ConsumerRecord<K, V>>> records = pollForFetches(timer);
      if (!records.isEmpty()) {
        // before returning the fetched records, we can send off the next round of fetches
        // and avoid block waiting for their responses to enable pipelining while the user
        // is handling the fetched records.
        //
        // NOTE: since the consumed position has already been updated, we must not allow
        // wakeups or any other errors to be triggered prior to returning the fetched records.
        if (fetcher.sendFetches() > 0 || client.hasPendingRequests()) {
          client.transmitSends();
        }
        // æ‹¦æˆªå™¨å¤„ç†æ•°æ® --> 4âƒ£ï¸
        return this.interceptors.onConsume(new ConsumerRecords<>(records));
      }
    } while (timer.notExpired());

    return ConsumerRecords.empty();
  } finally {
    release();
    this.kafkaConsumerMetrics.recordPollEnd(timer.currentTimeMs());
  }
}
```



2ï¼‰æ¶ˆè´¹è€…ç»„åˆå§‹åŒ–æµç¨‹

```java
boolean updateAssignmentMetadataIfNeeded(final Timer timer, final boolean waitForJoinGroup) {
  if (coordinator != null && !coordinator.poll(timer, waitForJoinGroup)) {
    return false;
  }

  return updateFetchPositions(timer);
}
---------------------------------------------------------
public boolean poll(Timer timer, boolean waitForJoinGroup) {
  // è·å–æœ€æ–°å…ƒæ•°æ®
  maybeUpdateSubscriptionMetadata();

  invokeCompletedOffsetCommitCallbacks();

  if (subscriptions.hasAutoAssignedPartitions()) {
    // å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ†åŒºåˆ†é…ç­–ç•¥  ç›´æ¥æŠ›å¼‚å¸¸
    if (protocol == null) {
      throw new IllegalStateException("User configured " + ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG +
                                      " to empty while trying to subscribe for group protocol to auto assign partitions");
    }
    // Always update the heartbeat last poll time so that the heartbeat thread does not leave the
    // group proactively due to application inactivity even if (say) the coordinator cannot be found.
    // 3så¿ƒè·³
    pollHeartbeat(timer.currentTimeMs());
    // â­ï¸ä¿è¯å’ŒCoordinatoræ­£å¸¸é€šä¿¡(å¯»æ‰¾æœåŠ¡å™¨ç«¯çš„coordinator)
    if (coordinatorUnknown() && !ensureCoordinatorReady(timer)) {
      return false;
    }

    // åˆ¤æ–­æ˜¯å¦éœ€è¦åŠ å…¥æ¶ˆè´¹è€…ç»„
    if (rejoinNeededOrPending()) {
      if (subscriptions.hasPatternSubscription()) {
        if (this.metadata.timeToAllowUpdate(timer.currentTimeMs()) == 0) {
          this.metadata.requestUpdate();
        }

        if (!client.ensureFreshMetadata(timer)) {
          return false;
        }
        maybeUpdateSubscriptionMetadata();
      }
      if (!ensureActiveGroup(waitForJoinGroup ? timer : time.timer(0L))) {
        timer.update(time.milliseconds());
        return false;
      }
    }
  } else {
    if (metadata.updateRequested() && !client.hasReadyNodes(timer.currentTimeMs())) {
      client.awaitMetadataUpdate(timer);
    }
  }

  // æ˜¯å¦è‡ªåŠ¨æäº¤offset
  maybeAutoCommitOffsetsAsync(timer.currentTimeMs());
  return true;
}
---------------------------------------------------------
protected synchronized boolean ensureCoordinatorReady(final Timer timer) {
  // å¦‚æœæ‰¾åˆ°coordinatorï¼Œç›´æ¥è¿”å›
  if (!coordinatorUnknown())
    return true;

  // å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå¾ªç¯ç»™æœåŠ¡å™¨ç«¯å‘é€è¯·æ±‚ï¼Œç›´åˆ°æ‰¾åˆ°coordinator
  do {
    if (fatalFindCoordinatorException != null) {
      final RuntimeException fatalException = fatalFindCoordinatorException;
      fatalFindCoordinatorException = null;
      throw fatalException;
    }
    // â­ï¸åˆ›å»ºä¸€ä¸ªæŸ¥æ‰¾Coordinator çš„è¯·æ±‚ å¹¶å‘é€
    final RequestFuture<Void> future = lookupCoordinator();
    // è·å–æœåŠ¡å™¨è¿”å›çš„ç»“æœ
    client.poll(future, timer);

    if (!future.isDone()) {
      // ran out of time
      break;
    }

    RuntimeException fatalException = null;

    if (future.failed()) {
      if (future.isRetriable()) {
        log.debug("Coordinator discovery failed, refreshing metadata", future.exception());
        client.awaitMetadataUpdate(timer);
      } else {
        fatalException = future.exception();
        log.info("FindCoordinator request hit fatal exception", fatalException);
      }
    } else if (coordinator != null && client.isUnavailable(coordinator)) {
      // we found the coordinator, but the connection has failed, so mark
      // it dead and backoff before retrying discovery
      markCoordinatorUnknown("coordinator unavailable");
      timer.sleep(rebalanceConfig.retryBackoffMs);
    }

    clearFindCoordinatorFuture();
    if (fatalException != null)
      throw fatalException;
  } while (coordinatorUnknown() && timer.notExpired());

  return !coordinatorUnknown();
}
---------------------------------------------------------
protected synchronized RequestFuture<Void> lookupCoordinator() {
  if (findCoordinatorFuture == null) {
    // find a node to ask about the coordinator
    Node node = this.client.leastLoadedNode();
    if (node == null) {
      log.debug("No broker available to send FindCoordinator request");
      return RequestFuture.noBrokersAvailable();
    } else {
      // â­ï¸æœ‰èŠ‚ç‚¹æ¥æ”¶æŸ¥æ‰¾Coordinatorè¯·æ±‚
      findCoordinatorFuture = sendFindCoordinatorRequest(node);
    }
  }
  return findCoordinatorFuture;
}
---------------------------------------------------------
private RequestFuture<Void> sendFindCoordinatorRequest(Node node) {
  // initiate the group metadata request
  log.debug("Sending FindCoordinator request to broker {}", node);

  // åˆ›å»ºå‘é€Coordinator è¯·æ±‚æ•°æ®ä¿¡æ¯
  FindCoordinatorRequestData data = new FindCoordinatorRequestData()
    .setKeyType(CoordinatorType.GROUP.id())
    .setKey(this.rebalanceConfig.groupId);

  // è¿›ä¸€æ­¥å°è£…
  FindCoordinatorRequest.Builder requestBuilder = new FindCoordinatorRequest.Builder(data);

  return client.send(node, requestBuilder)
    .compose(new FindCoordinatorResponseHandler());
}
```



3ï¼‰æ‹‰å–æ•°æ®

```java
private Map<TopicPartition, List<ConsumerRecord<K, V>>> pollForFetches(Timer timer) {
  long pollTimeout = coordinator == null ? timer.remainingMs() :
  Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs());

  // if data is available already, return it immediately
  // ç¬¬ä¸€æ¬¡æ‹‰å–ä¸åˆ°æ•°æ®
  final Map<TopicPartition, List<ConsumerRecord<K, V>>> records = fetcher.fetchedRecords();
  if (!records.isEmpty()) {
    return records;
  }

  // send any new fetches (won't resend pending fetches)
  // â­ï¸å‘é€è¯·æ±‚å¹¶æŠ“å–æ•°æ®
  fetcher.sendFetches();

  // We do not want to be stuck blocking in poll if we are missing some positions
  // since the offset lookup may be backing off after a failure

  // NOTE: the use of cachedSubscriptionHashAllFetchPositions means we MUST call
  // updateAssignmentMetadataIfNeeded before this method.
  if (!cachedSubscriptionHashAllFetchPositions && pollTimeout > retryBackoffMs) {
    pollTimeout = retryBackoffMs;
  }

  log.trace("Polling for fetches with timeout {}", pollTimeout);

  Timer pollTimer = time.timer(pollTimeout);
  client.poll(pollTimer, () -> {
    // since a fetch might be completed by the background thread, we need this poll condition
    // to ensure that we do not block unnecessarily in poll()
    return !fetcher.hasAvailableFetches();
  });
  timer.update(pollTimer.currentTimeMs());

  // â­ï¸æŠŠæ•°æ®æŒ‰ç…§åˆ†åŒºå°è£…å¥½åï¼Œä¸€æ¬¡å¤„ç†é»˜è®¤ 500 æ¡æ•°æ®
  return fetcher.fetchedRecords();
}

---------------------------------------------------------
// å‘é€è¯·æ±‚å¹¶æŠ“å–æ•°æ®
public synchronized int sendFetches() {
  // Update metrics in case there was an assignment change
  sensors.maybeUpdateAssignment(subscriptions);

  Map<Node, FetchSessionHandler.FetchRequestData> fetchRequestMap = prepareFetchRequests();
  for (Map.Entry<Node, FetchSessionHandler.FetchRequestData> entry : fetchRequestMap.entrySet()) {
    final Node fetchTarget = entry.getKey();
    final FetchSessionHandler.FetchRequestData data = entry.getValue();

    // maxWaitMs é»˜è®¤æ˜¯500ms
    // minBytes æœ€å°‘ä¸€æ¬¡æŠ“å–1ä¸ªå­—èŠ‚
    // maxBytes æœ€å¤šä¸€æ¬¡æŠ“å–å¤šå°‘æ•°æ®  é»˜è®¤50m
    final FetchRequest.Builder request = FetchRequest.Builder
      .forConsumer(this.maxWaitMs, this.minBytes, data.toSend())
      .isolationLevel(isolationLevel)
      .setMaxBytes(this.maxBytes)
      .metadata(data.metadata())
      .toForget(data.toForget())
      .rackId(clientRackId);

    if (log.isDebugEnabled()) {
      log.debug("Sending {} {} to broker {}", isolationLevel, data.toString(), fetchTarget);
    }
    // å‘é€æ‹‰å–æ•°æ®è¯·æ±‚
    RequestFuture<ClientResponse> future = client.send(fetchTarget, request);

    this.nodesWithPendingFetchRequests.add(entry.getKey().id());
    // ç›‘å¬æœåŠ¡å™¨ç«¯è¿”å›çš„æ•°æ®
    future.addListener(new RequestFutureListener<ClientResponse>() {
      @Override
      public void onSuccess(ClientResponse resp) {
        // æˆåŠŸæ¥æ”¶æœåŠ¡å™¨ç«¯æ•°æ®
        synchronized (Fetcher.this) {
          try {
            // è·å–æœåŠ¡å™¨ç«¯å“åº”æ•°æ®
            FetchResponse response = (FetchResponse) resp.responseBody();
            FetchSessionHandler handler = sessionHandler(fetchTarget.id());
            if (handler == null) {
              log.error("Unable to find FetchSessionHandler for node {}. Ignoring fetch response.",
                        fetchTarget.id());
              return;
            }
            if (!handler.handleResponse(response)) {
              return;
            }

            Set<TopicPartition> partitions = new HashSet<>(response.responseData().keySet());
            FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);

            for (Map.Entry<TopicPartition, FetchResponseData.PartitionData> entry : response.responseData().entrySet()) {
              TopicPartition partition = entry.getKey();
              FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition);
              if (requestData == null) {
                String message;
                if (data.metadata().isFull()) {
                  message = MessageFormatter.arrayFormat(
                    "Response for missing full request partition: partition={}; metadata={}",
                    new Object[]{partition, data.metadata()}).getMessage();
                } else {
                  message = MessageFormatter.arrayFormat(
                    "Response for missing session request partition: partition={}; metadata={}; toSend={}; toForget={}",
                    new Object[]{partition, data.metadata(), data.toSend(), data.toForget()}).getMessage();
                }

                // Received fetch response for missing session partition
                throw new IllegalStateException(message);
              } else {
                long fetchOffset = requestData.fetchOffset;
                FetchResponseData.PartitionData partitionData = entry.getValue();

                log.debug("Fetch {} at offset {} for partition {} returned fetch data {}",
                          isolationLevel, fetchOffset, partition, partitionData);

                Iterator<? extends RecordBatch> batches = FetchResponse.recordsOrFail(partitionData).batches().iterator();
                short responseVersion = resp.requestHeader().apiVersion();

                // æŠŠæ•°æ®æŒ‰ç…§åˆ†åŒºï¼Œæ·»åŠ åˆ°æ¶ˆæ¯é˜Ÿåˆ—é‡Œé¢
                completedFetches.add(new CompletedFetch(partition, partitionData,
                                                        metricAggregator, batches, fetchOffset, responseVersion));
              }
            }

            sensors.fetchLatency.record(resp.requestLatencyMs());
          } finally {
            nodesWithPendingFetchRequests.remove(fetchTarget.id());
          }
        }
      }

      @Override
      public void onFailure(RuntimeException e) {
        synchronized (Fetcher.this) {
          try {
            FetchSessionHandler handler = sessionHandler(fetchTarget.id());
            if (handler != null) {
              handler.handleError(e);
            }
          } finally {
            nodesWithPendingFetchRequests.remove(fetchTarget.id());
          }
        }
      }
    });

  }
  return fetchRequestMap.size();
}

---------------------------------------------------------
// æŠŠæ•°æ®æŒ‰ç…§åˆ†åŒºå°è£…å¥½åï¼Œä¸€æ¬¡å¤„ç†æœ€å¤§æ¡æ•°é»˜è®¤ 500 æ¡æ•°æ®
public Map<TopicPartition, List<ConsumerRecord<K, V>>> fetchedRecords() {
  Map<TopicPartition, List<ConsumerRecord<K, V>>> fetched = new HashMap<>();
  Queue<CompletedFetch> pausedCompletedFetches = new ArrayDeque<>();
  // æ¯æ¬¡å¤„ç†çš„æœ€å¤šæ¡æ•°æ˜¯500æ¡
  int recordsRemaining = maxPollRecords;

  try {
    while (recordsRemaining > 0) {
      if (nextInLineFetch == null || nextInLineFetch.isConsumed) {
        // ä»ç¼“å­˜ä¸­è·å–æ•°æ®
        CompletedFetch records = completedFetches.peek();
        // å¦‚æœæ²¡æœ‰æ•°æ®äº† å¯ä»¥é€€å‡ºå¾ªç¯
        if (records == null) break;

        if (records.notInitialized()) {
          try {
            nextInLineFetch = initializeCompletedFetch(records);
          } catch (Exception e) {
            FetchResponseData.PartitionData partition = records.partitionData;
            if (fetched.isEmpty() && FetchResponse.recordsOrFail(partition).sizeInBytes() == 0) {
              completedFetches.poll();
            }
            throw e;
          }
        } else {
          nextInLineFetch = records;
        }
        // ä»ç¼“å­˜ä¸­æ‹‰å–æ•°æ®
        completedFetches.poll();
      } else if (subscriptions.isPaused(nextInLineFetch.partition)) {
        log.debug("Skipping fetching records for assigned partition {} because it is paused", nextInLineFetch.partition);
        pausedCompletedFetches.add(nextInLineFetch);
        nextInLineFetch = null;
      } else {
        List<ConsumerRecord<K, V>> records = fetchRecords(nextInLineFetch, recordsRemaining);

        if (!records.isEmpty()) {
          TopicPartition partition = nextInLineFetch.partition;
          List<ConsumerRecord<K, V>> currentRecords = fetched.get(partition);
          if (currentRecords == null) {
            fetched.put(partition, records);
          } else {
            List<ConsumerRecord<K, V>> newRecords = new ArrayList<>(records.size() + currentRecords.size());
            newRecords.addAll(currentRecords);
            newRecords.addAll(records);
            fetched.put(partition, newRecords);
          }
          recordsRemaining -= records.size();
        }
      }
    }
  } catch (KafkaException e) {
    if (fetched.isEmpty())
      throw e;
  } finally {
    // add any polled completed fetches for paused partitions back to the completed fetches queue to be
    // re-evaluated in the next poll
    completedFetches.addAll(pausedCompletedFetches);
  }

  return fetched;
}
```



4ï¼‰æ‹¦æˆªå™¨å¤„ç†æ•°æ®

```java
public ConsumerRecords<K, V> onConsume(ConsumerRecords<K, V> records) {
  ConsumerRecords<K, V> interceptRecords = records;
  for (ConsumerInterceptor<K, V> interceptor : this.interceptors) {
    try {
      // æ¯ä¸ªæ‹¦æˆªå™¨éƒ½ä¼šå¯¹æ•°æ®è¿›è¡ŒåŠ å·¥
      interceptRecords = interceptor.onConsume(interceptRecords);
    } catch (Exception e) {
      // do not propagate interceptor exception, log and continue calling other interceptors
      log.warn("Error executing interceptor onConsume callback", e);
    }
  }
  return interceptRecords;
}
```



## æ¶ˆè´¹è€… **Offset** æäº¤

![image-20220512213303493](/assets/imgs/image-20220512213303493.png)

### æ‰‹åŠ¨åŒæ­¥æäº¤

1ï¼‰commitSync()

```java
public void commitSync(final Map<TopicPartition, OffsetAndMetadata> offsets, final Duration timeout) {
  acquireAndEnsureOpen();
  try {
    maybeThrowInvalidGroupIdException();
    offsets.forEach(this::updateLastSeenEpochIfNewer);
    // åŒæ­¥æäº¤ --> 2âƒ£ï¸
    if (!coordinator.commitOffsetsSync(new HashMap<>(offsets), time.timer(timeout))) {
      throw new TimeoutException("Timeout of " + timeout.toMillis() + "ms expired before successfully " +
                                 "committing offsets " + offsets);
    }
  } finally {
    release();
  }
}
```



2ï¼‰commitOffsetsSync()

```java
public boolean commitOffsetsSync(Map<TopicPartition, OffsetAndMetadata> offsets, Timer timer) {
  invokeCompletedOffsetCommitCallbacks();

  if (offsets.isEmpty())
    return true;

  do {
    if (coordinatorUnknown() && !ensureCoordinatorReady(timer)) {
      return false;
    }

    // å‘é€æäº¤è¯·æ±‚
    RequestFuture<Void> future = sendOffsetCommitRequest(offsets);
    client.poll(future, timer);

    // We may have had in-flight offset commits when the synchronous commit began. If so, ensure that
    // the corresponding callbacks are invoked prior to returning in order to preserve the order that
    // the offset commits were applied.
    invokeCompletedOffsetCommitCallbacks();

    // æäº¤æˆåŠŸ
    if (future.succeeded()) {
      if (interceptors != null)
        interceptors.onCommit(offsets);
      return true;
    }

    if (future.failed() && !future.isRetriable())
      throw future.exception();

    timer.sleep(rebalanceConfig.retryBackoffMs);
  } while (timer.notExpired());

  return false;
}
```



### æ‰‹åŠ¨å¼‚æ­¥æäº¤

1ï¼‰commitAsync()

```java
public void commitAsync(final Map<TopicPartition, OffsetAndMetadata> offsets, OffsetCommitCallback callback) {
  acquireAndEnsureOpen();
  try {
    maybeThrowInvalidGroupIdException();
    log.debug("Committing offsets: {}", offsets);
    offsets.forEach(this::updateLastSeenEpochIfNewer);
    // æäº¤offset --> 2âƒ£ï¸
    coordinator.commitOffsetsAsync(new HashMap<>(offsets), callback);
  } finally {
    release();
  }
}
```



2ï¼‰commitOffsetsAsync()

```java
public void commitOffsetsAsync(final Map<TopicPartition, OffsetAndMetadata> offsets, final OffsetCommitCallback callback) {
  invokeCompletedOffsetCommitCallbacks();

  if (!coordinatorUnknown()) {
    doCommitOffsetsAsync(offsets, callback);
  } else {
    // we don't know the current coordinator, so try to find it and then send the commit
    // or fail (we don't want recursive retries which can cause offset commits to arrive
    // out of order). Note that there may be multiple offset commits chained to the same
    // coordinator lookup request. This is fine because the listeners will be invoked in
    // the same order that they were added. Note also that AbstractCoordinator prevents
    // multiple concurrent coordinator lookup requests.
    pendingAsyncCommits.incrementAndGet();
    // ç›‘å¬æäº¤ offset çš„ç»“æœ
    lookupCoordinator().addListener(new RequestFutureListener<Void>() {
      @Override
      public void onSuccess(Void value) {
        pendingAsyncCommits.decrementAndGet();
        doCommitOffsetsAsync(offsets, callback);
        client.pollNoWakeup();
      }

      @Override
      public void onFailure(RuntimeException e) {
        pendingAsyncCommits.decrementAndGet();
        completedOffsetCommits.add(new OffsetCommitCompletion(callback, offsets,
                                                              new RetriableCommitFailedException(e)));
      }
    });
  }

  // ensure the commit has a chance to be transmitted (without blocking on its completion).
  // Note that commits are treated as heartbeats by the coordinator, so there is no need to
  // explicitly allow heartbeats through delayed task execution.
  client.pollNoWakeup();
}
```





# æœåŠ¡å™¨æºç 

![image-20220513212013048](/assets/imgs/image-20220513212013048.png)

1ï¼‰ç¨‹åºçš„å…¥å£ Kafka.scala

```scala
def main(args: Array[String]): Unit = {
  try {
    // è·å–ç›¸å…³å‚æ•°
    val serverProps = getPropsFromArgs(args)
    // åˆ›å»ºæœåŠ¡
    val server = buildServer(serverProps)

    try {
      if (!OperatingSystem.IS_WINDOWS && !Java.isIbmJdk)
      new LoggingSignalHandler().register()
    } catch {
      case e: ReflectiveOperationException =>
      warn("Failed to register optional signal handler that logs a message when the process is terminated " +
           s"by a signal. Reason for registration failure is: $e", e)
    }

    // attach shutdown handler to catch terminating signals as well as normal termination
    Exit.addShutdownHook("kafka-shutdown-hook", {
      try server.shutdown()
      catch {
        case _: Throwable =>
        fatal("Halting Kafka.")
        // Calling exit() can lead to deadlock as exit() can be called multiple times. Force exit.
        Exit.halt(1)
      }
    })

    // å¯åŠ¨æœåŠ¡
    try server.startup()
    catch {
      case _: Throwable =>
      // KafkaServer.startup() calls shutdown() in case of exceptions, so we invoke `exit` to set the status code
      fatal("Exiting Kafka.")
      Exit.exit(1)
    }

    server.awaitShutdown()
  }
  catch {
    case e: Throwable =>
    fatal("Exiting Kafka due to fatal exception", e)
    Exit.exit(1)
  }
  Exit.exit(0)
}
```

