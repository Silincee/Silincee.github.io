---
layout: post
title:  "Kafka源码"
date:   2022-05-10 15:40:06 +0800--
categories: [Kafka]
tags: [Kafka, ]  

---

# 前言

本文阅读的Kafka源码版本为：[kafka-3.0.0](http://kafka.apache.org/downloads)



# 生产者源码

## 生产者消息发送流程

在消息发送的过程中，涉及到了两个线程：**main** 线程和 **Sender** 线程。在 main 线程 中创建了一个双端队列 **RecordAccumulator**。main 线程通过分区器将消息发送给 RecordAccumulator， Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker。

![image-20220426144944892](/assets/imgs/image-20220426144944892.png)

- **batch.size**：只有数据积累到batch.size之后，sender才会发送数据，默认16k 
- **linger.ms**：如果数据迟迟未达到batch.size,sender等待linger.ms设置的时间。到了之后就会发送数据，单位ms，默认值是0ms，表示没有延迟。

应答acks：

- 0：生产者发送过来的数据，不需要等数据落盘应答。
- 1：生产者发送过来的数据，Leader 收到数据后应答。
- -1(all)：生产者发送过来的数据，Leader 和 ISR 队列 里面的所有节点收齐数据后应答。-1和 all 等价。



## 初始化

### 生产者main线程初始化

![image-20220510193208498](/assets/imgs/image-20220510193208498.png)



1）main线程中首先会通过构造器创建一个`KafkaProducer()`

```java
public KafkaProducer(Properties properties) {
  this(properties, null, null);
}
// 然后会依次调用以下构造器
public KafkaProducer(Properties properties, Serializer<K> keySerializer, Serializer<V> valueSerializer) {
  this(Utils.propsToMap(properties), keySerializer, valueSerializer);
}
public KafkaProducer(Map<String, Object> configs, Serializer<K> keySerializer, Serializer<V> valueSerializer) {
  this(new ProducerConfig(ProducerConfig.appendSerializerToConfig(configs, keySerializer, valueSerializer)),
       keySerializer, valueSerializer, null, null, null, Time.SYSTEM);
}
```



2）最终调用的KafkaProducer构造器

```java
@SuppressWarnings("unchecked")
KafkaProducer(ProducerConfig config,
              Serializer<K> keySerializer,
              Serializer<V> valueSerializer,
              ProducerMetadata metadata,
              KafkaClient kafkaClient,
              ProducerInterceptors<K, V> interceptors,
              Time time) {
  try {
    this.producerConfig = config;
    this.time = time;

    // 获取事务id
    String transactionalId = config.getString(ProducerConfig.TRANSACTIONAL_ID_CONFIG);

    // 获取客户端id
    this.clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);

    LogContext logContext;
    if (transactionalId == null)
      logContext = new LogContext(String.format("[Producer clientId=%s] ", clientId));
    else
      logContext = new LogContext(String.format("[Producer clientId=%s, transactionalId=%s] ", clientId, transactionalId));
    log = logContext.logger(KafkaProducer.class);
    log.trace("Starting the Kafka producer");

    Map<String, String> metricTags = Collections.singletonMap("client-id", clientId);
    MetricConfig metricConfig = new MetricConfig().samples(config.getInt(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG))
      .timeWindow(config.getLong(ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS)
      .recordLevel(Sensor.RecordingLevel.forName(config.getString(ProducerConfig.METRICS_RECORDING_LEVEL_CONFIG)))
      .tags(metricTags);
    List<MetricsReporter> reporters = config.getConfiguredInstances(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,
                                                                    MetricsReporter.class,
                                                                    Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
    // 监控kafka运行情况
    JmxReporter jmxReporter = new JmxReporter();
    jmxReporter.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)));
    reporters.add(jmxReporter);
    MetricsContext metricsContext = new KafkaMetricsContext(JMX_PREFIX,
                                                            config.originalsWithPrefix(CommonClientConfigs.METRICS_CONTEXT_PREFIX));
    this.metrics = new Metrics(metricConfig, reporters, time, metricsContext);
    // 获取分区器
    this.partitioner = config.getConfiguredInstance(
      ProducerConfig.PARTITIONER_CLASS_CONFIG,
      Partitioner.class,
      Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
    // 重试时间间隔参数配置，默认值100ms
    long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);
    // key和value的序列化
    if (keySerializer == null) {
      this.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                                                        Serializer.class);
      this.keySerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), true);
    } else {
      config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);
      this.keySerializer = keySerializer;
    }
    if (valueSerializer == null) {
      this.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                                                          Serializer.class);
      this.valueSerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), false);
    } else {
      config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);
      this.valueSerializer = valueSerializer;
    }

    // 拦截器处理（拦截器可以有多个）
    List<ProducerInterceptor<K, V>> interceptorList = (List) config.getConfiguredInstances(
      ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,
      ProducerInterceptor.class,
      Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));
    if (interceptors != null)
      this.interceptors = interceptors;
    else
      this.interceptors = new ProducerInterceptors<>(interceptorList);
    ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keySerializer,
                                                                                          valueSerializer, interceptorList, reporters);
    // 单条日志大小 默认1m
    this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);
    // 缓冲区大小 默认32m
    this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);
    // 压缩，默认是none
    this.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));

    this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);
    int deliveryTimeoutMs = configureDeliveryTimeout(config, log);

    this.apiVersions = new ApiVersions();
    this.transactionManager = configureTransactionState(config, logContext);
    // 缓冲区对象 默认是32m 参数列表如下：
    // 批次大小 默认16k
    // 压缩方式，默认是none
    // liner.ms 默认是0
    //  内存池
    this.accumulator = new RecordAccumulator(logContext,
                                             config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),
                                             this.compressionType,
                                             lingerMs(config),
                                             retryBackoffMs,
                                             deliveryTimeoutMs,
                                             metrics,
                                             PRODUCER_METRIC_GROUP_NAME,
                                             time,
                                             apiVersions,
                                             transactionManager,
                                             new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));

    // 连接上kafka集群地址
    List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
      config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG),
      config.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG));
    // 获取元数据
    if (metadata != null) {
      this.metadata = metadata;
    } else {
      this.metadata = new ProducerMetadata(retryBackoffMs,
                                           config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),
                                           config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),
                                           logContext,
                                           clusterResourceListeners,
                                           Time.SYSTEM);
      this.metadata.bootstrap(addresses);
    }
    this.errors = this.metrics.sensor("errors");
		
    this.sender = newSender(logContext, kafkaClient, this.metadata);
    String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;
    // 把sender线程放到后台
    this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
    // ⭐️启动sender线程
    this.ioThread.start();
    config.logUnused();
    AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());
    log.debug("Kafka producer started");
  } catch (Throwable t) {
    // call close methods if internal objects are already constructed this is to prevent resource leak. see KAFKA-2121
    close(Duration.ofMillis(0), true);
    // now propagate the exception
    throw new KafkaException("Failed to construct kafka producer", t);
  }
}
```



---



### 生产者sender线程初始化

![image-20220510193229658](/assets/imgs/image-20220510193229658.png)

1）main线程初始化中，调用`newSender(logContext, kafkaClient, this.metadata);`来到sender线程初始化

```java
Sender newSender(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata) {
  // 缓存请求的个数 默认是5个
  int maxInflightRequests = configureInflightRequests(producerConfig);
  // 请求超时时间，默认30s
  int requestTimeoutMs = producerConfig.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);
  ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(producerConfig, time, logContext);
  ProducerMetrics metricsRegistry = new ProducerMetrics(this.metrics);
  Sensor throttleTimeSensor = Sender.throttleTimeSensor(metricsRegistry.senderMetrics);

  // 创建一个客户端对象
  // clientId  客户端id
  // maxInflightRequests  缓存请求的个数 默认是5个
  // RECONNECT_BACKOFF_MS_CONFIG 重试时间
  // RECONNECT_BACKOFF_MAX_MS_CONFIG 总的重试时间
  // 发送缓冲区大小send.buffer.bytes  默认128kb
  // 接收数据缓存 receive.buffer.bytes 默认是32kb
  KafkaClient client = kafkaClient != null ? kafkaClient : new NetworkClient(
    new Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),
                 this.metrics, time, "producer", channelBuilder, logContext),
    metadata,
    clientId,
    maxInflightRequests,
    producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),
    producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),
    producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),
    producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),
    requestTimeoutMs,
    producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),
    producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG),
    time,
    true,
    apiVersions,
    throttleTimeSensor,
    logContext);
  // 0 ：生产者发送过来，不需要应答；  1 ：leader收到，应答；  -1 ：leader和isr队列里面所有的都收到了应答
  short acks = configureAcks(producerConfig, log);
  // 创建sender线程
  return new Sender(logContext,
                    client,
                    metadata,
                    this.accumulator,
                    maxInflightRequests == 1,
                    producerConfig.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),
                    acks,
                    producerConfig.getInt(ProducerConfig.RETRIES_CONFIG),
                    metricsRegistry.senderMetrics,
                    time,
                    requestTimeoutMs,
                    producerConfig.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG),
                    this.transactionManager,
                    apiVersions);
}
```



2）Sender 对象被放到了一个线程中启动，所有需要点击 `newSender()`方法中的 Sender，并 找到 sender 对象中的 `run()`方法。

```java
public void run() {
  log.debug("Starting Kafka producer I/O thread.");

  // main loop, runs until close is called
  while (running) {
    try {
      // sender 线程从缓冲区准备拉取数据，刚启动拉不到数据
      runOnce();
    } catch (Exception e) {
      log.error("Uncaught error in kafka producer I/O thread: ", e);
    }
  }
  ...
}
```



## 生产者发送数据到缓冲区

![image-20220511222901498](/assets/imgs/image-20220511222901498.png)

### 发送总体流程

1）`KafkaProducer.send()`方法

```java
public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {
  // intercept the record, which can be potentially modified; this method does not throw exceptions
  // 拦截器相关操作
  ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);
  return doSend(interceptedRecord, callback);
}
```



2）其中的 `onSend()`方法，进行拦截器相关处理。

```java
public ProducerRecord<K, V> onSend(ProducerRecord<K, V> record) {
  ProducerRecord<K, V> interceptRecord = record;
  for (ProducerInterceptor<K, V> interceptor : this.interceptors) {
    try {
      // 拦截器对数据进行加工
      interceptRecord = interceptor.onSend(interceptRecord);
    } catch (Exception e) {
      // do not propagate interceptor exception, log and continue calling other interceptors
      // be careful not to throw exception from here
      if (record != null)
        log.warn("Error executing interceptor onSend callback for topic: {}, partition: {}", record.topic(), record.partition(), e);
      else
        log.warn("Error executing interceptor onSend callback", e);
    }
  }
  return interceptRecord;
}
```



3）从拦截器处理中返回，点击 `doSend()`方法。

```java
private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {
  TopicPartition tp = null;
  try {
    throwIfProducerClosed();
    // first make sure the metadata for the topic is available
    long nowMs = time.milliseconds();
    ClusterAndWaitTime clusterAndWaitTime;
    try {
      // 从 Kafka 拉取元数据。maxBlockTimeMs 表示最多能等待多长时间。
      clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);
    } catch (KafkaException e) {
      if (metadata.isClosed())
        throw new KafkaException("Producer closed while send in progress", e);
      throw e;
    }
    nowMs += clusterAndWaitTime.waitedOnMetadataMs;
    // 剩余时间 = 最多能等待时间 - 用了多少时间;
    long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);
    // 更新集群元数据
    Cluster cluster = clusterAndWaitTime.cluster;
    // 序列化相关操作
    byte[] serializedKey;
    try {
      serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());
    } catch (ClassCastException cce) {
      throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
                                       " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
                                       " specified in key.serializer", cce);
    }
    byte[] serializedValue;
    try {
      serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());
    } catch (ClassCastException cce) {
      throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
                                       " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
                                       " specified in value.serializer", cce);
    }
    // 🤔分区操作(根据元数据信息)
    int partition = partition(record, serializedKey, serializedValue, cluster);
    tp = new TopicPartition(record.topic(), partition);

    setReadOnly(record.headers());
    Header[] headers = record.headers().toArray();

    int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),
                                                                       compressionType, serializedKey, serializedValue, headers);
    // 🤔保证数据大小能够传输(序列化后的  压缩后的)，发送消息的大小是否超过最大值，默认是1m
    ensureValidRecordSize(serializedSize);

    long timestamp = record.timestamp() == null ? nowMs : record.timestamp();
    if (log.isTraceEnabled()) {
      log.trace("Attempting to append record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition);
    }
    // 消息发送的回调函数
    // producer callback will make sure to call both 'callback' and interceptor callback
    Callback interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);

    if (transactionManager != null && transactionManager.isTransactional()) {
      transactionManager.failIfNotReadyForSend();
    }
    // 🤔accumulator缓存  追加数据  result是是否添加成功的结果
    RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,
                                                                     serializedValue, headers, interceptCallback, remainingWaitMs, true, nowMs);

    if (result.abortForNewBatch) {
      int prevPartition = partition;
      partitioner.onNewBatch(record.topic(), cluster, prevPartition);
      partition = partition(record, serializedKey, serializedValue, cluster);
      tp = new TopicPartition(record.topic(), partition);
      if (log.isTraceEnabled()) {
        log.trace("Retrying append due to new batch creation for topic {} partition {}. The old partition was {}", record.topic(), partition, prevPartition);
      }
      // producer callback will make sure to call both 'callback' and interceptor callback
      interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);

      result = accumulator.append(tp, timestamp, serializedKey,
                                  serializedValue, headers, interceptCallback, remainingWaitMs, false, nowMs);
    }

    if (transactionManager != null && transactionManager.isTransactional())
      transactionManager.maybeAddPartitionToTransaction(tp);
    // 批次大小已经满了 获取有一个新批次创建
    if (result.batchIsFull || result.newBatchCreated) {
      log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);
      // 唤醒发送线程
      this.sender.wakeup();
    }
    return result.future;
    // handling exceptions and record the errors;
    // for API exceptions return them in the future,
    // for other exceptions throw directly
  } catch (ApiException e) {
    log.debug("Exception occurred during message send:", e);
    if (callback != null)
      callback.onCompletion(null, e);
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    return new FutureFailure(e);
  } catch (InterruptedException e) {
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    throw new InterruptException(e);
  } catch (KafkaException e) {
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    throw e;
  } catch (Exception e) {
    // we notify interceptor about all exceptions, since onSend is called before anything else in this method
    this.interceptors.onSendError(record, tp, e);
    throw e;
  }
}
```



### 分区选择

1）发送总体流程中以下代码为分区选择相关流程：

```java
// 分区操作
int partition = partition(record, serializedKey, serializedValue, cluster);
tp = new TopicPartition(record.topic(), partition);

// partition()方法具体如下：
private int partition(ProducerRecord<K, V> record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) {
  Integer partition = record.partition();
  // 如果指定分区，按照指定分区配置
  return partition != null ?
    partition :
  // 分区器选择分区
  partitioner.partition( // 点击 partition，跳转到 Partitioner 接口，选择默认的分区器 DefaultPartitioner --> 2⃣️
    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);
}
```



2）点击 partition，跳转到 Partitioner 接口，选择默认的分区器 DefaultPartitioner

```java
public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster,
                     int numPartitions) {
  // 没有指定key
  if (keyBytes == null) {
    // 按照粘性分区处理 --> 3⃣️
    return stickyPartitionCache.partition(topic, cluster);
  }
  // 如果指定key,按照key的hashcode值 对分区数求模
  // hash the keyBytes to choose a partition
  return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
}
---------------------------------------------------------
// 3⃣️没有指定key和分区的处理方式 stickyPartitionCache.partition(topic, cluster);
public int partition(String topic, Cluster cluster) {
  Integer part = indexCache.get(topic);
  if (part == null) {
    return nextPartition(topic, cluster, -1); // --> 4⃣️
  }
  return part;
}
---------------------------------------------------------
// 4⃣️nextPartition(topic, cluster, -1);
public int nextPartition(String topic, Cluster cluster, int prevPartition) {
  List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
  Integer oldPart = indexCache.get(topic);
  Integer newPart = oldPart;
  // Check that the current sticky partition for the topic is either not set or that the partition that 
  // triggered the new batch matches the sticky partition that needs to be changed.
  if (oldPart == null || oldPart == prevPartition) {
    List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);
    if (availablePartitions.size() < 1) {
      Integer random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
      newPart = random % partitions.size();
    } else if (availablePartitions.size() == 1) {
      newPart = availablePartitions.get(0).partition();
    } else {
      while (newPart == null || newPart.equals(oldPart)) {
        int random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
        newPart = availablePartitions.get(random % availablePartitions.size()).partition();
      }
    }
    // Only change the sticky partition if it is null or prevPartition matches the current sticky partition.
    if (oldPart == null) {
      indexCache.putIfAbsent(topic, newPart);
    } else {
      indexCache.replace(topic, prevPartition, newPart);
    }
    return indexCache.get(topic);
  }
  return indexCache.get(topic);
}
```



### 发送消息大小校验

发送总体流程中的ensureValidRecordSize(serializedSize)方法涉及到发送消息大小校验操作

```java
private void ensureValidRecordSize(int size) {
  // 单条信息最大值 maxRequestSize 1m
  if (size > maxRequestSize)
    throw new RecordTooLargeException("The message is " + size +
                                      " bytes when serialized which is larger than " + maxRequestSize + ", which is the value of the " +
                                      ProducerConfig.MAX_REQUEST_SIZE_CONFIG + " configuration.");
  // 缓冲区内存总大小，默认32m
  if (size > totalMemorySize)
    throw new RecordTooLargeException("The message is " + size +
                                      " bytes when serialized which is larger than the total memory buffer you have configured with the " +
                                      ProducerConfig.BUFFER_MEMORY_CONFIG +
                                      " configuration.");
}
```





### 内存/缓存池

发送总体流程中的`RecordAccumulator.RecordAppendResult result = accumulator.append(...);`

```java
public RecordAppendResult append(TopicPartition tp,
                                 long timestamp,
                                 byte[] key,
                                 byte[] value,
                                 Header[] headers,
                                 Callback callback,
                                 long maxTimeToBlock,
                                 boolean abortOnNewBatch,
                                 long nowMs) throws InterruptedException {
  // We keep track of the number of appending thread to make sure we do not miss batches in
  // abortIncompleteBatches().
  appendsInProgress.incrementAndGet();
  ByteBuffer buffer = null;
  if (headers == null) headers = Record.EMPTY_HEADERS;
  try {
    // check if we have an in-progress batch
    // 获取或者创建一个队列（按照每个主题的分区）
    Deque<ProducerBatch> dq = getOrCreateDeque(tp);
    synchronized (dq) {
      if (closed)
        throw new KafkaException("Producer closed while send in progress");
      // 尝试向队列里面添加数据(没有分配内存、批次对象，所以失败)
      RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);
      if (appendResult != null)
        return appendResult;
    }

    // we don't have an in-progress record batch try to allocate a new batch
    if (abortOnNewBatch) {
      // Return a result that will cause another call to append.
      return new RecordAppendResult(null, false, false, true);
    }

    byte maxUsableMagic = apiVersions.maxUsableProduceMagic();
    // this.batchSize 默认16k    数据大小17k
    int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));
    log.trace("Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms", size, tp.topic(), tp.partition(), maxTimeToBlock);
    // 申请内存  内存池根据批次大小(默认16k)和消息大小中最大值，分配内存   😎双端队列 
    buffer = free.allocate(size, maxTimeToBlock);

    // Update the current time in case the buffer allocation blocked above.
    nowMs = time.milliseconds();
    synchronized (dq) {
      // Need to check if producer is closed again after grabbing the dequeue lock.
      if (closed)
        throw new KafkaException("Producer closed while send in progress");
      // 尝试向队列里面添加数据(有内存，但是没有批次对象)
      RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);
      if (appendResult != null) {
        // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often...
        return appendResult;
      }
      // 封装内存buffer
      MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);
      // 根据内存大小封装批次(有内存、有批次对象)
      ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, nowMs);
      // 尝试向队列里面添加数据
      FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers,
                                                                           callback, nowMs));
      // 把新创建的批次放到队列末尾
      dq.addLast(batch);
      incomplete.add(batch);

      // Don't deallocate this buffer in the finally block as it's being used in the record batch
      buffer = null;
      return new RecordAppendResult(future, dq.size() > 1 || batch.isFull(), true, false);
    }
  } finally {
    // 如果发生异常，释放内存
    if (buffer != null)
      free.deallocate(buffer);
    appendsInProgress.decrementAndGet();
  }
}
```



## **sender** 线程发送数据

![image-20220512000146708](/assets/imgs/image-20220512000146708.png)



1）进入 sender 发送线程的 `run()`方法。

```java
public void run() {
  log.debug("Starting Kafka producer I/O thread.");

  // main loop, runs until close is called
  while (running) {
    try {
      // sender 线程从缓冲区准备拉取数据，刚启动拉不到数据 --> 2⃣️
      runOnce();
    } catch (Exception e) {
      log.error("Uncaught error in kafka producer I/O thread: ", e);
    }
  }
  ...
}
```



2）`runOnce()`方法

```java
void runOnce() {
  // 如果是事务操作，按照如下处理
  if (transactionManager != null) {
    try {
      transactionManager.maybeResolveSequences();

      // do not continue sending if the transaction manager is in a failed state
      if (transactionManager.hasFatalError()) {
        RuntimeException lastError = transactionManager.lastError();
        if (lastError != null)
          maybeAbortBatches(lastError);
        // 获取服务器端响应 -->7⃣️
        client.poll(retryBackoffMs, time.milliseconds()); 
        return;
      }

      // Check whether we need a new producerId. If so, we will enqueue an InitProducerId
      // request which will be sent below
      transactionManager.bumpIdempotentEpochAndResetIdIfNeeded();

      if (maybeSendAndPollTransactionalRequest()) {
        return;
      }
    } catch (AuthenticationException e) {
      // This is already logged as error, but propagated here to perform any clean ups.
      log.trace("Authentication exception while processing transactional request", e);
      transactionManager.authenticationFailed(e);
    }
  }

  long currentTimeMs = time.milliseconds();
  // 将准备好的数据发送到服务器端 --> 3⃣️
  long pollTimeout = sendProducerData(currentTimeMs);
  // 获取发送结果
  client.poll(pollTimeout, currentTimeMs);
}
```



3）获取要发送数据的细节 `sendProducerData()`

```java
private long sendProducerData(long now) {
  // 获取元数据
  Cluster cluster = metadata.fetch();
  // get the list of partitions with data ready to send
  // 判断32m缓存是否准备好(linger.ms) --> 4⃣️
  RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);

  // 如果Leader信息不知道，是不能发送数据的
  // if there are any partitions whose leaders are not known yet, force metadata update
  if (!result.unknownLeaderTopics.isEmpty()) {
    // The set of topics with unknown leader contains topics with leader election pending as well as
    // topics which may have expired. Add the topic again to metadata to ensure it is included
    // and request metadata update, since there are messages to send to the topic.
    for (String topic : result.unknownLeaderTopics)
      this.metadata.add(topic, now);

    log.debug("Requesting metadata update due to unknown leader topics from the batched records: {}",
              result.unknownLeaderTopics);
    this.metadata.requestUpdate();
  }

  // remove any nodes we aren't ready to send to
  // 删除掉没有准备好发送的数据
  Iterator<Node> iter = result.readyNodes.iterator();
  long notReadyTimeout = Long.MAX_VALUE;
  while (iter.hasNext()) {
    Node node = iter.next();
    if (!this.client.ready(node, now)) {
      iter.remove();
      notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));
    }
  }

  // create produce requests
  // 发往同一个 broker 节点的数据，打包为一个请求批次 --> 5⃣️
  Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);
  addToInflightBatches(batches);
  if (guaranteeMessageOrder) {
    // Mute all the partitions drained
    for (List<ProducerBatch> batchList : batches.values()) {
      for (ProducerBatch batch : batchList)
        this.accumulator.mutePartition(batch.topicPartition);
    }
  }

  accumulator.resetNextBatchExpiryTime();
  List<ProducerBatch> expiredInflightBatches = getExpiredInflightBatches(now);
  List<ProducerBatch> expiredBatches = this.accumulator.expiredBatches(now);
  expiredBatches.addAll(expiredInflightBatches);

  // Reset the producer id if an expired batch has previously been sent to the broker. Also update the metrics
  // for expired batches. see the documentation of @TransactionState.resetIdempotentProducerId to understand why
  // we need to reset the producer id here.
  if (!expiredBatches.isEmpty())
    log.trace("Expired {} batches in accumulator", expiredBatches.size());
  for (ProducerBatch expiredBatch : expiredBatches) {
    String errorMessage = "Expiring " + expiredBatch.recordCount + " record(s) for " + expiredBatch.topicPartition
      + ":" + (now - expiredBatch.createdMs) + " ms has passed since batch creation";
    failBatch(expiredBatch, new TimeoutException(errorMessage), false);
    if (transactionManager != null && expiredBatch.inRetry()) {
      // This ensures that no new batches are drained until the current in flight batches are fully resolved.
      transactionManager.markSequenceUnresolved(expiredBatch);
    }
  }
  sensors.updateProduceRequestMetrics(batches);

  // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately
  // loop and try sending more data. Otherwise, the timeout will be the smaller value between next batch expiry
  // time, and the delay time for checking data availability. Note that the nodes may have data that isn't yet
  // sendable due to lingering, backing off, etc. This specifically does not include nodes with sendable data
  // that aren't ready to send since they would cause busy looping.
  long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);
  pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now);
  pollTimeout = Math.max(pollTimeout, 0);
  if (!result.readyNodes.isEmpty()) {
    log.trace("Nodes with data ready to send: {}", result.readyNodes);
    // if some partitions are already ready to be sent, the select time would be 0;
    // otherwise if some partition already has some data accumulated but not ready yet,
    // the select time will be the time difference between now and its linger expiry time;
    // otherwise the select time will be the time difference between now and the metadata expiry time;
    pollTimeout = 0;
  }
  // 发送请求 --> 6⃣️
  sendProduceRequests(batches, now);
  return pollTimeout;
}
```



4）判断32m缓存是否准备好

```java
public ReadyCheckResult ready(Cluster cluster, long nowMs) {
  Set<Node> readyNodes = new HashSet<>();
  long nextReadyCheckDelayMs = Long.MAX_VALUE;
  Set<String> unknownLeaderTopics = new HashSet<>();

  boolean exhausted = this.free.queued() > 0;
  for (Map.Entry<TopicPartition, Deque<ProducerBatch>> entry : this.batches.entrySet()) {
    Deque<ProducerBatch> deque = entry.getValue();
    synchronized (deque) {
      // When producing to a large number of partitions, this path is hot and deques are often empty.
      // We check whether a batch exists first to avoid the more expensive checks whenever possible.
      ProducerBatch batch = deque.peekFirst();
      if (batch != null) {
        TopicPartition part = entry.getKey();
        Node leader = cluster.leaderFor(part);
        if (leader == null) {
          // This is a partition for which leader is not known, but messages are available to send.
          // Note that entries are currently not removed from batches when deque is empty.
          unknownLeaderTopics.add(part.topic());
        } else if (!readyNodes.contains(leader) && !isMuted(part)) {
          long waitedTimeMs = batch.waitedTimeMs(nowMs);
          // 如果不是第一次拉取，  且等待时间小于重试时间 默认100ms ,backingOff=true
          boolean backingOff = batch.attempts() > 0 && waitedTimeMs < retryBackoffMs;
          // 如果backingOff是true 取retryBackoffMs； 如果不是第一次拉取取lingerMs，默认0
          long timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;
          // 批次大小满足发送条件
          boolean full = deque.size() > 1 || batch.isFull();
          // 如果等待的时间超过了 timeToWaitMs，expired=true，表示可以发送数据
          boolean expired = waitedTimeMs >= timeToWaitMs;
          boolean transactionCompleting = transactionManager != null && transactionManager.isCompleting();
          // full linger.ms
          boolean sendable = full
            || expired
            || exhausted
            || closed
            || flushInProgress()
            || transactionCompleting;
          if (sendable && !backingOff) {
            readyNodes.add(leader);
          } else {
            long timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, 0);
            // Note that this results in a conservative estimate since an un-sendable partition may have
            // a leader that will later be found to have sendable data. However, this is good enough
            // since we'll just wake up and then sleep again for the remaining time.
            nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);
          }
        }
      }
    }
  }
  return new ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);
}
```



5）发往同一个 **broker** 节点的数据，打包为一个请求批次。

```java
public Map<Integer, List<ProducerBatch>> drain(Cluster cluster, Set<Node> nodes, int maxSize, long now) {
  if (nodes.isEmpty())
    return Collections.emptyMap();

  Map<Integer, List<ProducerBatch>> batches = new HashMap<>();
  for (Node node : nodes) {
    List<ProducerBatch> ready = drainBatchesForOneNode(cluster, node, maxSize, now);
    batches.put(node.id(), ready);
  }
  return batches;
}
```



6）发送请求

```java
private void sendProduceRequest(long now, int destination, short acks, int timeout, List<ProducerBatch> batches) {
  if (batches.isEmpty())
    return;

  final Map<TopicPartition, ProducerBatch> recordsByPartition = new HashMap<>(batches.size());

  // find the minimum magic version used when creating the record sets
  byte minUsedMagic = apiVersions.maxUsableProduceMagic();
  for (ProducerBatch batch : batches) {
    if (batch.magic() < minUsedMagic)
      minUsedMagic = batch.magic();
  }
  ProduceRequestData.TopicProduceDataCollection tpd = new ProduceRequestData.TopicProduceDataCollection();
  for (ProducerBatch batch : batches) {
    TopicPartition tp = batch.topicPartition;
    MemoryRecords records = batch.records();

    // down convert if necessary to the minimum magic used. In general, there can be a delay between the time
    // that the producer starts building the batch and the time that we send the request, and we may have
    // chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use
    // the new message format, but found that the broker didn't support it, so we need to down-convert on the
    // client before sending. This is intended to handle edge cases around cluster upgrades where brokers may
    // not all support the same message format version. For example, if a partition migrates from a broker
    // which is supporting the new magic version to one which doesn't, then we will need to convert.
    if (!records.hasMatchingMagic(minUsedMagic))
      records = batch.records().downConvert(minUsedMagic, 0, time).records();
    ProduceRequestData.TopicProduceData tpData = tpd.find(tp.topic());
    if (tpData == null) {
      tpData = new ProduceRequestData.TopicProduceData().setName(tp.topic());
      tpd.add(tpData);
    }
    tpData.partitionData().add(new ProduceRequestData.PartitionProduceData()
                               .setIndex(tp.partition())
                               .setRecords(records));
    recordsByPartition.put(tp, batch);
  }

  String transactionalId = null;
  if (transactionManager != null && transactionManager.isTransactional()) {
    transactionalId = transactionManager.transactionalId();
  }

  ProduceRequest.Builder requestBuilder = ProduceRequest.forMagic(minUsedMagic,
                                                                  new ProduceRequestData()
                                                                  .setAcks(acks)
                                                                  .setTimeoutMs(timeout)
                                                                  .setTransactionalId(transactionalId)
                                                                  .setTopicData(tpd));
  RequestCompletionHandler callback = response -> handleProduceResponse(response, recordsByPartition, time.milliseconds());

  String nodeId = Integer.toString(destination);
  // 创建发送请求对象
  ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0,
                                                        requestTimeoutMs, callback);
  // 发送请求
  client.send(clientRequest, now);
  log.trace("Sent produce request to {}: {}", nodeId, requestBuilder);
}
---------------------------------------------------------
public void send(ClientRequest request, long now) {
  doSend(request, false, now);
}
---------------------------------------------------------
private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long now, AbstractRequest request) {
  String destination = clientRequest.destination();
  RequestHeader header = clientRequest.makeHeader(request.version());
  if (log.isDebugEnabled()) {
    log.debug("Sending {} request with header {} and timeout {} to node {}: {}",
              clientRequest.apiKey(), header, clientRequest.requestTimeoutMs(), destination, request);
  }
  Send send = request.toSend(header);
  InFlightRequest inFlightRequest = new InFlightRequest(
    clientRequest,
    header,
    isInternalRequest,
    request,
    send,
    now);
  // 添加请求到inflint
  this.inFlightRequests.add(inFlightRequest);
  // 发送数据
  selector.send(new NetworkSend(clientRequest.destination(), send));
}
```



7）获取服务器端响应

```java
public List<ClientResponse> poll(long timeout, long now) {
  ensureActive();

  if (!abortedSends.isEmpty()) {
    // If there are aborted sends because of unsupported version exceptions or disconnects,
    // handle them immediately without waiting for Selector#poll.
    List<ClientResponse> responses = new ArrayList<>();
    handleAbortedSends(responses);
    completeResponses(responses);
    return responses;
  }

  long metadataTimeout = metadataUpdater.maybeUpdate(now);
  try {
    this.selector.poll(Utils.min(timeout, metadataTimeout, defaultRequestTimeoutMs));
  } catch (IOException e) {
    log.error("Unexpected error during I/O", e);
  }

  // process completed actions
  // 获取发送后的响应
  long updatedNow = this.time.milliseconds();
  List<ClientResponse> responses = new ArrayList<>();
  handleCompletedSends(responses, updatedNow);
  handleCompletedReceives(responses, updatedNow);
  handleDisconnections(responses, updatedNow);
  handleConnections();
  handleInitiateApiVersionRequests(updatedNow);
  handleTimedOutConnections(responses, updatedNow);
  handleTimedOutRequests(responses, updatedNow);
  completeResponses(responses);

  return responses;
}
```







# 消费者源码

