---
layout: post
title:  "Kafka学习笔记 性能调优"
date:   2022-05-09 14:40:06 +0800--
categories: [Kafka]
tags: [Kafka, ]  
---

#  硬件配置选择

1）场景说明：100 万日活，每人每天 100 条日志。

- 每天总共的日志条数是 100 万 * 100 条 = 1 亿条。
- 1 亿/24 小时/60 分/60 秒 = 1150 条/每秒钟。
- 每条日志大小:0.5k - 2k(取 1k)。
- 1150 条/每秒钟 * 1k ≈ 1m/s 。
- 高峰期每秒钟:1150 条 * 20 倍 = 23000 条。 每秒数据量:20MB/s。



2）服务器台数选择

服务器台数= 2 * (生产者峰值生产速率 * 副本 / 100) + 1 = 2 * (20m/s * 2 / 100) + 1=3台 

建议 3 台服务器。



3）磁盘选择

kafka 底层主要是==顺序写==，固态硬盘和机械硬盘的顺序写速度差不多。 建议选择普通的机械硬盘。

- 每天总数据量:1 亿条 * 1k ≈ 100g
- 100g* 副本2* 保存时间3天 /0.7 ≈ 1T 建议三台服务器硬盘总大小，大于等于 1T。



4）内存选择

Kafka 内存组成:堆内存 + 页缓存

Kafka 堆内存建议每个节点:10g ~ 15g。可在在 `kafka-server-start.sh` 中修改:

```shell
if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then 
	export KAFKA_HEAP_OPTS="-Xmx10G -Xms10G"
fi
```

> 如何评估当前堆内存使用情况

```shell
# 查看 Kafka 进程号
jps
# 查看GC情况 主要看YGC:年轻代垃圾回收次数;
jstat -gc 75030 ls 10
# 也可以根据 Kafka 进程号，查看 Kafka 的堆内存
jmap -heap 75030
```

页缓存：页缓存是 Linux 系统服务器的内存。我们只需要保证 1 个 segment(1g)中 25%的数据在内存中就好。

- 每个节点页缓存大小 = (分区数 * 1g * 25%) / 节点数。例如 10 个分区，页缓存大小 =(10\*1g\*25%)/3 ≈ 1g
- 建议服务器内存大于等于 11G。



5）CPU选择

num.io.threads = 8 负责写磁盘的线程数，整个参数值要占总核数的 50%。 

num.replica.fetchers = 1 副本拉取线程数，这个参数占总核数的 50%的 1/3。

num.network.threads = 3 数据传输线程数，这个参数占总核数的 50%的 2/3。

建议 32 个 cpu core。



6）网络选择

网络带宽 = 峰值吞吐量 ≈ 20MB/s 选择千兆网卡即可。

- 100Mbps 单位是 bit;10M/s 单位是 byte ; 1byte = 8bit，100Mbps/8 = 12.5M/s。
-  一般百兆的网卡(100Mbps )、千兆的网卡(1000Mbps)、万兆的网卡(10000Mbps)。



# Kafka生产者调优

在消息发送的过程中，涉及到了两个线程：**main** 线程和 **Sender** 线程。在 main 线程 中创建了一个双端队列 **RecordAccumulator**。main 线程通过分区器将消息发送给 RecordAccumulator， Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker。

- **batch.size**：只有数据积累到batch.size之后，sender才会发送数据，默认16k 
- **linger.ms**：如果数据迟迟未达到batch.size,sender等待linger.ms设置的时间。到了之后就会发送数据，单位ms，默认值是0ms，表示没有延迟。

![image-20220426144944892](/assets/imgs/image-20220426144944892.png)

## 核心参数配置

| 参数名称                              | 描述                                                         |
| ------------------------------------- | ------------------------------------------------------------ |
| bootstrap.servers                     | 生产者连接集群所需的 broker 地址清单。例如 hadoop102:9092,hadoop103:9092,hadoop104:9092，可以设置 1 个或者多个，中间用逗号隔开。注意这里并非需要所有的 broker 地址，因为生产者从给定的 broker 里查找到其他 broker 信息。 |
| key.serializer 和 value.serializer    | 指定发送消息的 key 和 value 的序列化类型。一定要写 全类名。  |
| buffer.memory                         | RecordAccumulator 缓冲区总大小，==默认 32m。==               |
| batch.size                            | 缓冲区一批数据最大值，==默认 16k==。适当增加该值，可 以提高吞吐量，但是如果该值设置太大，会导致数据 传输延迟增加。 |
| linger.ms                             | 如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。单位 ms，==默认值是 0ms==，表示没 有延迟。生产环境建议该值大小为 5-100ms 之间。 |
| acks                                  | 0：生产者发送过来的数据，不需要等数据落盘应答。 1：生产者发送过来的数据，Leader 收到数据后应答。 -1(all)：生产者发送过来的数据，Leader 和 ISR 队列 里面的所有节点收齐数据后应答。==默认值是-1，-1 和 all 是等价的。== |
| max.in.flight.requests.per.connection | 允许最多没有返回 ack 的次数，==默认为 5==，开启幂等性 要保证该值是 1-5 的数字。 |
| retries                               | 当消息发送出现错误的时候，系统会重发消息。retries 表示重试次数。==默认是 int 最大值，2147483647。== 如果设置了重试，还想保证消息的有序性，需要设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1 否则在重试此失败消息的时候，其他的消息可能发送成功了。 |
| retry.backoff.ms                      | **两次重试之间的时间间隔，默认是 100ms。**                   |
| enable.idempotence                    | 是否开启幂等性，==默认 true==，开启幂等性。                  |
| compression.type                      | 生产者发送的所有数据的压缩方式。==默认是 none==，也 就是不压缩。 支持压缩类型:none、gzip、snappy、lz4 和 zstd。 |



## 生产者调优

> [Kafka生产者生产经验](http://www.silince.cn/2022/04/22/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%94%9F%E4%BA%A7%E8%80%85/#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C)

### 生产者如何提高吞吐量

| 参数名称         | 描述                                                         |
| ---------------- | ------------------------------------------------------------ |
| buffer.memory    | RecordAccumulator 缓冲区总大小，==默认 32m。==               |
| batch.size       | 缓冲区一批数据最大值，==默认 16k==。适当增加该值，可 以提高吞吐量，但是如果该值设置太大，会导致数据传 输延迟增加。 |
| linger.ms        | 如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。单位 ms，==默认值是 0ms==，表示没有 延迟。生产环境建议该值大小为 5-100ms 之间。 |
| compression.type | 生产者发送的所有数据的压缩方式。==默认是 none==，也就 是不压缩。 支持压缩类型:==none、gzip、snappy、lz4 和 zstd。== |



### 数据可靠性

**至少一次(At Least Once)= ACK 级别设置为-1 + 分区副本大于等于 2 + ISR 里应 答的最小副本数量大于等于 2**

| 参数名称 | 描述                                                         |
| -------- | ------------------------------------------------------------ |
| acks     | 0:生产者发送过来的数据，不需要等数据落盘应答。 <br />1:生产者发送过来的数据，Leader 收到数据后应答。 <br />-1(all):生产者发送过来的数据，Leader+和 isr 队列 里面的所有节点收齐数据后应答。默认值是-1，-1 和 all 是等价的。 |



### 数据去重

1）开启幂等性

| 参数名称           | 描述                                        |
| ------------------ | ------------------------------------------- |
| enable.idempotence | 是否开启幂等性，默认 true，表示开启幂等性。 |

2）Kafka事务

```java
// 1 初始化事务
void initTransactions();
// 2 开启事务
void beginTransaction() throws ProducerFencedException;
// 3 在事务内提交已经消费的偏移量(主要用于消费者)
void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offsets,
                              ProducerFencedException;
                              String consumerGroupId) throws
// 4 提交事务
void commitTransaction() throws ProducerFencedException;
// 5 放弃事务(类似于回滚事务的操作)
void abortTransaction() throws ProducerFencedException;
```

### 数据有序

单分区内，有序(还需有其他条件); 多分区，分区与分区间无序;

![image-20220428171145835](/assets/imgs/image-20220428171145835.png)

> 🤔单分区==数据乱序问题==？因为sender线程将数据发送到kafka集群的过程中存在重试机制。

1）kafka在1.x版本之前保证数据单分区有序，条件如下:

- **max.in.flight.requests.per.connection**=1(不需要考虑是否开启幂等性)。

2）kafka在**1.x及以后版本**保证数据单分区有序，条件如下: 

- 未开启幂等性：**max.in.flight.requests.per.connection(该参数指定了生产者在收到服务器响应之前可以发送多少个消息)**需要设置为**1**
- 开启幂等性：**max.in.flight.requests.per.connection**需要设置==小于等于5==。因为在kafka1.x以后，启用幂等后，**kafka服务端**会缓存producer发来的最近==5==个request的元数据， 故无论如何，都可以保证最近5个request的数据都是有序的(根据序列号)

![image-20220428171919457](/assets/imgs/image-20220428171919457.png)





# Kafka Broker调优

![image-20220428182003980](/assets/imgs/image-20220428182003980.png)

## 核心参数配置

| 参数名称                                | 描述                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| replica.lag.time.max.ms                 | ISR中，如果Follower长时间未向Leader发送通 信请求或同步数据，则该 Follower 将被踢出 ISR。 该时间阈值，==默认 30s。== |
| auto.leader.rebalance.enable            | ==默认是 true==。 自动 Leader Partition 平衡。建议关闭。     |
| leader.imbalance.per.broker.percentage  | ==默认是 10%==。每个 broker 允许的不平衡的 leader 的比率。如果每个 broker 超过了这个值，控制器 会触发 leader 的平衡。 |
| leader.imbalance.check.interval.seconds | ==默认值 300 秒==。检查 leader 负载是否平衡的间隔 时间。     |
| log.segment.bytes                       | Kafka 中 log 日志是分成一块块存储的，此配置 是指 log 日志划分 成块的大小，==默认值 1G。== |
| log.index.interval.bytes                | ==默认 4kb==，kafka 里面每当写入了 4kb 大小的日志 (.log)，然后就往 index 文件里面记录一个索引。 |
| log.retention.hours                     | Kafka 中数据保存的时间，==默认 7 天==。                      |
| log.retention.minutes                   | Kafka 中数据保存的时间，==分钟级别==，默认关闭。             |
| log.retention.ms                        | Kafka 中数据保存的时间，==毫秒级别==，默认关闭。             |
| log.retention.check.interval.ms         | 检查数据是否保存超时的间隔，==默认是 5 分钟==。              |
| log.retention.bytes                     | ==默认等于-1==，表示无穷大。超过设置的所有日志 总大小，删除最早的 segment。 |
| log.cleanup.policy                      | ==默认是 delete==，表示所有数据启用删除策略; 如果设置值为 compact，表示所有数据启用压缩 策略。 |
| num.io.threads                          | ==默认是 8==。负责写磁盘的线程数。整个参数值要 占总核数的 50%。 |
| num.replica.fetchers                    | ==默认是 1==。副本拉取线程数，这个参数占总核数 的 50%的 1/3  |
| num.network.threads                     | ==默认是 3==。数据传输线程数，这个参数占总核数 的 50%的 2/3 。 |
| log.flush.interval.messages             | 强制页缓存刷写到磁盘的条数，默认是 long 的最 大值，9223372036854775807。==一般不建议修改==， 交给系统自己管理。 |
| log.flush.interval.ms                   | 每隔多久，刷数据到磁盘，默认是 null。==一般不建议修改==，交给系统自己管理。 |



## 服役新节点**/**退役旧节点

```shell
# 1.创建一个要均衡的主题。
vim topics-to-move.json
{
"topics": [
{"topic": "first"}
],
"version": 1
# 2.生成一个负载均衡的计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --topics-to-move-json-file topics-to-move.json --broker-list "0,1,2,3" --generate
# 3.创建副本存储计划(所有副本存储在 broker0、broker1、broker2、broker3 中)。
vim increase-replication-factor.json
# 4.执行副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute
# 5.验证副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --verify
```



## 增加分区

```shell
# 修改分区数(注意:分区数只能增加，不能减少)
bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3
```





## 增加副本因子

在生产环境当中，由于某个主题的重要等级需要提升，我们考虑增加副本。副本数的增加需要先制定计划，然后根据计划执行。

```shell
# 1.创建 topic
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 3 --replication-factor 1 -- topic four
# 2.手动增加副本存储。创建副本存储计划(所有副本都指定存储在 broker0、broker1、broker2 中)。
vim increase-replication-factor.json
{"version":1,"partitions":[{"topic":"four","partition":0,"replicas":[0,1,2]},{"topic":"four","partition":1,"replicas":[0,1,2]},{"t opic":"four","partition":2,"replicas":[0,1,2]}]}
# 3.执行副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute
```



## 手动调整分区副本存储

在生产环境中，每台服务器的配置和性能不一致，但是Kafka只会根据自己的代码规则创建对应的分区副本，就会导致个别服务器存储压力较大。所有需要手动调整分区副本的存储。

![image-20220503232436895](/assets/imgs/image-20220503232436895.png)

手动调整分区副本存储的步骤如下:

```shell
# 1.创建一个新的 topic，名称为 three。
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 4 --replication-factor 2 -- topic three
# 2.查看分区副本存储情况。
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic three
# 3.创建副本存储计划(所有副本都指定存储在 broker0、broker1 中)。
vim increase-replication-factor.json
{
   "version":1,
   "partitions":[{"topic":"three","partition":0,"replicas":[0,1]}, 
                 {"topic":"three","partition":1,"replicas":[0,1]}, 
                 {"topic":"three","partition":2,"replicas":[1,0]}, 
                 {"topic":"three","partition":3,"replicas":[1,0]}]
}
# 4.执行副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute
# 5.验证副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --verify
# 6.查看分区副本存储情况。
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic three
```

## **Leader Partition** 负载平衡

正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，**会导致Leader Partition过于集中在其他少部分几台broker上**，这会导致少数几台broker的读写请求压力过高，其他宕机的 broker重启之后都是follower partition，读写请求很低，**造成集群负载不均衡**。

![image-20220503232919857](/assets/imgs/image-20220503232919857.png)

| 参数名称                                | 描述                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| auto.leader.rebalance.enable            | 默认是 true。 自动 Leader Partition 平衡。生产环 境中，==leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭。== |
| leader.imbalance.per.broker.percentage  | 默认是 10%。每个 broker 允许的不平衡的 leader 的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡。 |
| leader.imbalance.check.interval.seconds | 默认值 300 秒。检查 leader 负载是否平衡的间隔 时间。         |



## 自动创建主题

如果 broker 端配置参数 auto.create.topics.enable 设置为 true(默认值是 true)，那么当生产者向一个未创建的主题发送消息时，会自动创建一个分区数为 num.partitions(默认值为 1)、副本因子为 default.replication.factor(默认值为 1)的主题。除此之外，当一个消费者开始从未知主题中读取消息时，或者当任意一个客户端向未知主题发送元数据请求时，都会自动创建一个相应主题。这种创建主题的方式是非预期的，增加了主题管理和维护的难度。 ==生产环境建议将该参数设置为 false。==

```shell
# 向一个没有提前创建 five 主题发送数据
bin/kafka-console-producer.sh -- bootstrap-server localhost:9092 --topic five
>hello world
# 查看 five 主题的详情
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic five
```



# **Kafka**消费者调优

