---
layout: post
title:  "Kafka学习笔记 性能调优"
date:   2022-05-09 14:40:06 +0800--
categories: [Kafka]
tags: [Kafka, ]  
---

#  硬件配置选择

1）场景说明：100 万日活，每人每天 100 条日志。

- 每天总共的日志条数是 100 万 * 100 条 = 1 亿条。
- 1 亿/24 小时/60 分/60 秒 = 1150 条/每秒钟。
- 每条日志大小:0.5k - 2k(取 1k)。
- 1150 条/每秒钟 * 1k ≈ 1m/s 。
- 高峰期每秒钟:1150 条 * 20 倍 = 23000 条。 每秒数据量:20MB/s。



2）服务器台数选择

服务器台数= 2 * (生产者峰值生产速率 * 副本 / 100) + 1 = 2 * (20m/s * 2 / 100) + 1=3台 

==建议 3 台服务器。==



3）磁盘选择

kafka 底层主要是`顺序写`，固态硬盘和机械硬盘的顺序写速度差不多。 建议选择普通的机械硬盘。

- 每天总数据量:1 亿条 * 1k ≈ 100g
- 100g* 副本2* 保存时间3天 /0.7 ≈ 1T 建议三台服务器硬盘总大小，大于等于 1T。



4）内存选择

Kafka 内存组成:堆内存 + 页缓存

Kafka 堆内存建议每个节点:10g ~ 15g。可在在 `kafka-server-start.sh` 中修改:

```shell
if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then 
	export KAFKA_HEAP_OPTS="-Xmx10G -Xms10G"
fi
```

> 如何评估当前堆内存使用情况

```shell
# 查看 Kafka 进程号
jps
# 查看GC情况 主要看YGC:年轻代垃圾回收次数;
jstat -gc 75030 ls 10
# 也可以根据 Kafka 进程号，查看 Kafka 的堆内存
jmap -heap 75030
```

页缓存：页缓存是 Linux 系统服务器的内存。我们只需要保证 1 个 segment(1g)中 25%的数据在内存中就好。

- 每个节点页缓存大小 = (分区数 * 1g * 25%) / 节点数。例如 10 个分区，页缓存大小 =(10\*1g\*25%)/3 ≈ 1g
- 建议服务器内存大于等于 11G。



5）CPU选择

num.io.threads = 8 负责写磁盘的线程数，整个参数值要占总核数的 50%。 

num.replica.fetchers = 1 副本拉取线程数，这个参数占总核数的 50%的 1/3。

num.network.threads = 3 数据传输线程数，这个参数占总核数的 50%的 2/3。

建议 32 个 cpu core。



6）网络选择

网络带宽 = 峰值吞吐量 ≈ 20MB/s 选择千兆网卡即可。

- 100Mbps 单位是 bit;10M/s 单位是 byte ; 1byte = 8bit，100Mbps/8 = 12.5M/s。
-  一般百兆的网卡(100Mbps )、千兆的网卡(1000Mbps)、万兆的网卡(10000Mbps)。



# Kafka生产者调优

在消息发送的过程中，涉及到了两个线程：**main** 线程和 **Sender** 线程。在 main 线程 中创建了一个双端队列 **RecordAccumulator**。main 线程通过分区器将消息发送给 RecordAccumulator， Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker。

- **batch.size**：只有数据积累到batch.size之后，sender才会发送数据，默认16k 
- **linger.ms**：如果数据迟迟未达到batch.size,sender等待linger.ms设置的时间。到了之后就会发送数据，单位ms，默认值是0ms，表示没有延迟。

![image-20220426144944892](/assets/imgs/image-20220426144944892.png)

## 核心参数配置

| 参数名称                              | 描述                                                         |
| ------------------------------------- | ------------------------------------------------------------ |
| bootstrap.servers                     | 生产者连接集群所需的 broker 地址清单。例如 hadoop102:9092,hadoop103:9092,hadoop104:9092，可以设置 1 个或者多个，中间用逗号隔开。注意这里并非需要所有的 broker 地址，因为生产者从给定的 broker 里查找到其他 broker 信息。 |
| key.serializer 和 value.serializer    | 指定发送消息的 key 和 value 的序列化类型。一定要写 全类名。  |
| buffer.memory                         | RecordAccumulator 缓冲区总大小，`默认 32m。`                 |
| batch.size                            | 缓冲区一批数据最大值，`默认 16k`。适当增加该值，可 以提高吞吐量，但是如果该值设置太大，会导致数据 传输延迟增加。 |
| linger.ms                             | 如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。单位 ms，`默认值是 0ms`，表示没 有延迟。生产环境建议该值大小为 5-100ms 之间。 |
| acks                                  | 0：生产者发送过来的数据，不需要等数据落盘应答。 1：生产者发送过来的数据，Leader 收到数据后应答。 -1(all)：生产者发送过来的数据，Leader 和 ISR 队列 里面的所有节点收齐数据后应答。`默认值是-1，-1 和 all 是等价的。` |
| max.in.flight.requests.per.connection | 允许最多没有返回 ack 的次数，`默认为 5`，开启幂等性 要保证该值是 1-5 的数字。 |
| retries                               | 当消息发送出现错误的时候，系统会重发消息。retries 表示重试次数。`默认是 int 最大值，2147483647。` 如果设置了重试，还想保证消息的有序性，需要设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1 否则在重试此失败消息的时候，其他的消息可能发送成功了。 |
| retry.backoff.ms                      | **两次重试之间的时间间隔，默认是 100ms。**                   |
| enable.idempotence                    | 是否开启幂等性，`默认 true`，开启幂等性。                    |
| compression.type                      | 生产者发送的所有数据的压缩方式。`默认是 none`，也 就是不压缩。 支持压缩类型:none、gzip、snappy、lz4 和 zstd。 |



## 生产者调优

> [Kafka生产者生产经验](http://www.silince.cn/2022/04/22/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%94%9F%E4%BA%A7%E8%80%85/#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C)

### 生产者如何提高吞吐量

| 参数名称         | 描述                                                         |
| ---------------- | ------------------------------------------------------------ |
| buffer.memory    | RecordAccumulator 缓冲区总大小，`默认 32m。`                 |
| batch.size       | 缓冲区一批数据最大值，`默认 16k`。适当增加该值，可 以提高吞吐量，但是如果该值设置太大，会导致数据传 输延迟增加。 |
| linger.ms        | 如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。单位 ms，`默认值是 0ms`，表示没有 延迟。生产环境建议该值大小为 5-100ms 之间。 |
| compression.type | 生产者发送的所有数据的压缩方式。`默认是 none`，也就 是不压缩。 支持压缩类型:`none、gzip、snappy、lz4 和 zstd。` |



### 数据可靠性

**至少一次(At Least Once)= ACK 级别设置为-1 + 分区副本大于等于 2 + ISR 里应答的最小副本数量大于等于 2**

| 参数名称 | 描述                                                         |
| -------- | ------------------------------------------------------------ |
| acks     | 0:生产者发送过来的数据，不需要等数据落盘应答。 <br />1:生产者发送过来的数据，Leader 收到数据后应答。 <br />-1(all):生产者发送过来的数据，Leader+和 isr 队列 里面的所有节点收齐数据后应答。默认值是-1，-1 和 all 是等价的。 |



### 数据去重

1）开启幂等性

| 参数名称           | 描述                                        |
| ------------------ | ------------------------------------------- |
| enable.idempotence | 是否开启幂等性，默认 true，表示开启幂等性。 |

2）Kafka事务

```java
// 1 初始化事务
void initTransactions();
// 2 开启事务
void beginTransaction() throws ProducerFencedException;
// 3 在事务内提交已经消费的偏移量(主要用于消费者)
void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offsets,
                              ProducerFencedException;
                              String consumerGroupId) throws
// 4 提交事务
void commitTransaction() throws ProducerFencedException;
// 5 放弃事务(类似于回滚事务的操作)
void abortTransaction() throws ProducerFencedException;
```

### 数据有序

单分区内，有序(还需有其他条件); 多分区，分区与分区间无序;

![image-20220428171145835](/assets/imgs/image-20220428171145835.png)

> 🤔单分区`数据乱序问题`？因为sender线程将数据发送到kafka集群的过程中存在重试机制。

1）kafka在1.x版本之前保证数据单分区有序，条件如下:

- **max.in.flight.requests.per.connection**=1(不需要考虑是否开启幂等性)。

2）kafka在**1.x及以后版本**保证数据单分区有序，条件如下: 

- 未开启幂等性：**max.in.flight.requests.per.connection(该参数指定了生产者在收到服务器响应之前可以发送多少个消息)**需要设置为**1**
- 开启幂等性：**max.in.flight.requests.per.connection**需要设置`小于等于5`。因为在kafka1.x以后，启用幂等后，**kafka服务端**会缓存producer发来的最近`5`个request的元数据， 故无论如何，都可以保证最近5个request的数据都是有序的(根据序列号)

![image-20220428171919457](/assets/imgs/image-20220428171919457.png)





# Kafka Broker调优

![image-20220428182003980](/assets/imgs/image-20220428182003980.png)

## 核心参数配置

| 参数名称                                | 描述                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| replica.lag.time.max.ms                 | ISR中，如果Follower长时间未向Leader发送通 信请求或同步数据，则该 Follower 将被踢出 ISR。 该时间阈值，`默认 30s。` |
| auto.leader.rebalance.enable            | `默认是 true`。 自动 Leader Partition 平衡。建议关闭。       |
| leader.imbalance.per.broker.percentage  | `默认是 10%`。每个 broker 允许的不平衡的 leader 的比率。如果每个 broker 超过了这个值，控制器 会触发 leader 的平衡。 |
| leader.imbalance.check.interval.seconds | `默认值 300 秒`。检查 leader 负载是否平衡的间隔 时间。       |
| log.segment.bytes                       | Kafka 中 log 日志是分成一块块存储的，此配置 是指 log 日志划分 成块的大小，`默认值 1G。` |
| log.index.interval.bytes                | `默认 4kb`，kafka 里面每当写入了 4kb 大小的日志 (.log)，然后就往 index 文件里面记录一个索引。 |
| log.retention.hours                     | Kafka 中数据保存的时间，`默认 7 天`。                        |
| log.retention.minutes                   | Kafka 中数据保存的时间，`分钟级别`，默认关闭。               |
| log.retention.ms                        | Kafka 中数据保存的时间，`毫秒级别`，默认关闭。               |
| log.retention.check.interval.ms         | 检查数据是否保存超时的间隔，`默认是 5 分钟`。                |
| log.retention.bytes                     | `默认等于-1`，表示无穷大。超过设置的所有日志 总大小，删除最早的 segment。 |
| log.cleanup.policy                      | `默认是 delete`，表示所有数据启用删除策略; 如果设置值为 compact，表示所有数据启用压缩 策略。 |
| num.io.threads                          | `默认是 8`。负责写磁盘的线程数。整个参数值要 占总核数的 50%。 |
| num.replica.fetchers                    | `默认是 1`。副本拉取线程数，这个参数占总核数 的 50%的 1/3    |
| num.network.threads                     | `默认是 3`。数据传输线程数，这个参数占总核数 的 50%的 2/3 。 |
| log.flush.interval.messages             | 强制页缓存刷写到磁盘的条数，默认是 long 的最 大值，9223372036854775807。`一般不建议修改`， 交给系统自己管理。 |
| log.flush.interval.ms                   | 每隔多久，刷数据到磁盘，默认是 null。`一般不建议修改`，交给系统自己管理。 |



## 服役新节点**/**退役旧节点

```shell
# 1.创建一个要均衡的主题。
vim topics-to-move.json
{
"topics": [
{"topic": "first"}
],
"version": 1
# 2.生成一个负载均衡的计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --topics-to-move-json-file topics-to-move.json --broker-list "0,1,2,3" --generate
# 3.创建副本存储计划(所有副本存储在 broker0、broker1、broker2、broker3 中)。
vim increase-replication-factor.json
# 4.执行副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute
# 5.验证副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --verify
```



## 增加分区

```shell
# 修改分区数(注意:分区数只能增加，不能减少)
bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3
```





## 增加副本因子

在生产环境当中，由于某个主题的重要等级需要提升，我们考虑增加副本。副本数的增加需要先制定计划，然后根据计划执行。

```shell
# 1.创建 topic
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 3 --replication-factor 1 -- topic four
# 2.手动增加副本存储。创建副本存储计划(所有副本都指定存储在 broker0、broker1、broker2 中)。
vim increase-replication-factor.json
{"version":1,"partitions":[{"topic":"four","partition":0,"replicas":[0,1,2]},{"topic":"four","partition":1,"replicas":[0,1,2]},{"t opic":"four","partition":2,"replicas":[0,1,2]}]}
# 3.执行副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute
```



## 手动调整分区副本存储

在生产环境中，每台服务器的配置和性能不一致，但是Kafka只会根据自己的代码规则创建对应的分区副本，就会导致个别服务器存储压力较大。所有需要手动调整分区副本的存储。

![image-20220503232436895](/assets/imgs/image-20220503232436895.png)

手动调整分区副本存储的步骤如下:

```shell
# 1.创建一个新的 topic，名称为 three。
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 4 --replication-factor 2 -- topic three
# 2.查看分区副本存储情况。
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic three
# 3.创建副本存储计划(所有副本都指定存储在 broker0、broker1 中)。
vim increase-replication-factor.json
{
   "version":1,
   "partitions":[{"topic":"three","partition":0,"replicas":[0,1]}, 
                 {"topic":"three","partition":1,"replicas":[0,1]}, 
                 {"topic":"three","partition":2,"replicas":[1,0]}, 
                 {"topic":"three","partition":3,"replicas":[1,0]}]
}
# 4.执行副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute
# 5.验证副本存储计划。
bin/kafka-reassign-partitions.sh -- bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --verify
# 6.查看分区副本存储情况。
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic three
```

## **Leader Partition** 负载平衡

正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，**会导致Leader Partition过于集中在其他少部分几台broker上**，这会导致少数几台broker的读写请求压力过高，其他宕机的 broker重启之后都是follower partition，读写请求很低，**造成集群负载不均衡**。

![image-20220503232919857](/assets/imgs/image-20220503232919857.png)

| 参数名称                                | 描述                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| auto.leader.rebalance.enable            | 默认是 true。 自动 Leader Partition 平衡。生产环 境中，`leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭。` |
| leader.imbalance.per.broker.percentage  | 默认是 10%。每个 broker 允许的不平衡的 leader 的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡。 |
| leader.imbalance.check.interval.seconds | 默认值 300 秒。检查 leader 负载是否平衡的间隔 时间。         |



## 自动创建主题

如果 broker 端配置参数 auto.create.topics.enable 设置为 true(默认值是 true)，那么当生产者向一个未创建的主题发送消息时，会自动创建一个分区数为 num.partitions(默认值为 1)、副本因子为 default.replication.factor(默认值为 1)的主题。除此之外，当一个消费者开始从未知主题中读取消息时，或者当任意一个客户端向未知主题发送元数据请求时，都会自动创建一个相应主题。这种创建主题的方式是非预期的，增加了主题管理和维护的难度。 `生产环境建议将该参数设置为 false。`

```shell
# 向一个没有提前创建 five 主题发送数据
bin/kafka-console-producer.sh -- bootstrap-server localhost:9092 --topic five
>hello world
# 查看 five 主题的详情
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic five
```



# **Kafka**消费者调优

1）消费者组初始化流程

coordinator：辅助实现消费者组的初始化和分区的分配。

- coordinator节点选择=groupid的nashcode值%50(\_consumer_offsets的分区数量)_
- 例如：groupid的hashcodef值=1，1%50=1，那么\_consumer_offsets主题的1号分区，在哪个broker上，**就选择这个节点的coordinator 作为这个消费者组的老大。消费者组下的所有的消费者提交offset的时候就往这个分区去提交offset。**

![image-20220506165927636](/assets/imgs/image-20220506165927636.png)

⚠️触发再平衡的两个条件：

- 每个消费者都会和coordinator保持心跳(默认3s)，一旦超时 (session.timeout.ms=45s)，该消费者会被移除，并触发再平衡;
- 或者消费者处理消息的时间过长(超过max.poll.interval.ms5分钟)，也会触发再平衡



2）消费流程

1. 创建COnsumerNetworkClient，接收消费者的消费请求sendFetches
2. ConsumerNetworkClient通过send方法拉去broker中的消息，broker也有相应onSuccess回调方法
3. 把拉去的消息放入completedFetches队列，消费者会按批次拉取数据
4. 反序列化/拦截器/处理数据

![image-20220506180242291](/assets/imgs/image-20220506180242291.png)

## 核心参数配置

| 参数名称                               | 描述                                                         |
| -------------------------------------- | ------------------------------------------------------------ |
| bootstrap.servers                      | 向 Kafka 集群建立初始连接用到的 host/port 列表。             |
| key.deserializer 和 value.deserializer | 指定接收消息的 key 和 value 的反序列化类型。一定要写全 类名。 |
| group.id                               | 标记消费者所属的消费者组。                                   |
| enable.auto.commit                     | `默认值为 true`，消费者会自动周期性地向服务器提交偏移量。    |
| auto.commit.interval.ms                | 如果设置了 enable.auto.commit 的值为 true， 则该值定义了 消费者偏移量向 Kafka 提交的频率，`默认 5s。` |
| auto.offset.reset                      | 当 Kafka 中没有初始偏移量或当前偏移量在服务器中不存在 (如数据被删除了)，该如何处理? earliest:自动重置偏 移量到最早的偏移量。 `latest:默认，自动重置偏移量为最新的偏移量`。 none:如果消费组原来的(previous)偏移量不 存在，则向消费者抛异常。 anything:向消费者抛异常。 |
| offsets.topic.num.partitions           | __consumer_offsets 的分区数，默认是 50 个分区。不建议修 改。 |
| heartbeat.interval.ms                  | Kafka 消费者和 coordinator 之间的心跳时间，`默认 3s`。 该条目的值必须小于 session.timeout.ms ，也不应该高于 session.timeout.ms 的 1/3。`不建议修改`。 |
| session.timeout.ms                     | Kafka 消费者和 coordinator 之间连接超时时间，`默认 45s`。超 过该值，该消费者被移除，消费者组执行再平衡。 |
| max.poll.interval.ms                   | 消费者处理消息的最大时长，`默认是 5 分钟`。超过该值，该 消费者被移除，消费者组执行再平衡。 |
| fetch.min.bytes                        | `默认 1 个字节`。消费者获取服务器端一批消息最小的字节数      |
| fetch.max.wait.ms                      | `默认 500ms`。如果没有从服务器端获取到一批数据的最小字 节数。该时间到，仍然会返回数据。 |
| fetch.max.bytes                        | `默认Default: 52428800(50m)`。消费者获取服务器端一批 消息最大的字节数。如果服务器端一批次的数据大于该值 (50m)仍然可以拉取回来这批数据，因此，这不是一个绝 对最大值。一批次的大小受 message.max.bytes (broker config)or max.message.bytes (topic config)影响。 |
| max.poll.records                       | 一次 poll 拉取数据返回消息的最大条数，`默认是 500 条。`      |



## 消费者再平衡

> [分区的分配以及再平衡](http://www.silince.cn/2022/05/05/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B6%88%E8%B4%B9%E8%80%85/#%E5%88%86%E5%8C%BA%E7%9A%84%E5%88%86%E9%85%8D%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1)

| 参数名称                      | 描述                                                         |
| ----------------------------- | ------------------------------------------------------------ |
| heartbeat.interval.ms         | Kafka 消费者和 coordinator 之间的心跳时间，`默认 3s`。 该条目的值必须小于 session.timeout.ms，也不应该高于 session.timeout.ms 的 1/3。 |
| session.timeout.ms            | Kafka 消费者和 coordinator 之间连接超时时间，`默认 45s`。 超过该值，该消费者被移除，消费者组执行再平衡。 |
| max.poll.interval.ms          | 消费者处理消息的最大时长，`默认是 5 分钟`。超过该值，该 消费者被移除，消费者组执行再平衡。 |
| partition.assignment.strategy | 消费者分区分配策略，默认策略是 Range + CooperativeSticky。Kafka 可以同时使用多个分区分配策略。 可以选择的策略包括:`Range、RoundRobin、Sticky、 CooperativeSticky` |



## 指定 **Offset** 消费

[指定 **offset** 消费](http://www.silince.cn/2022/05/05/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B6%88%E8%B4%B9%E8%80%85/#%E6%8C%87%E5%AE%9A-offset-%E6%B6%88%E8%B4%B9)



##  指定时间消费

[指定时间消费](http://www.silince.cn/2022/05/05/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B6%88%E8%B4%B9%E8%80%85/#%E6%8C%87%E5%AE%9A%E6%97%B6%E9%97%B4%E6%B6%88%E8%B4%B9)



## 消费者如何提高吞吐量

1）如果是Kafka消费能力不足，则可以考虑增 加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）

2）如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间<生产速度）， 使处理的数据小于生产的数据，也会造成数据积压。

![image-20220508161128405](/assets/imgs/image-20220508161128405.png)

| 参数名称         | 描述                                                         |
| ---------------- | ------------------------------------------------------------ |
| fetch.max.bytes  | 默认Default: 52428800(50m)。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值 (50m)仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受 message.max.bytes (broker config)or max.message.bytes (topic config)影响。 |
| max.poll.records | 一次 poll 拉取数据返回消息的最大条数，默认是 500 条          |



# Kafka整体调优总结

## 如何提升吞吐量

1）提升生产端吞吐量

- **buffer.memory**:发送消息的缓冲区大小，默认值是 32m，可以增加到 64m。 
- **batch.size**:默认是 16k。如果 batch 设置太小，会导致频繁网络请求，吞吐量下降;如果 batch 太大，会导致一条消息需要等待很久才能被发送出去，增加网络延时。 
- **linger.ms**，这个值默认是 0，意思就是消息必须立即被发送。一般设置一个 5-100 毫秒。如果 linger.ms 设置的太小，会导致频繁网络请求，吞吐量下降;如果 linger.ms 太长，会导致一条消息需要等待很久才能被发送出去，增加网络延时。 
- **compression.type**:默认是 none，不压缩，但是也可以使用 lz4 压缩，效率还是不错的，压缩之后可以减小数据量，提升吞吐量，但是会加大 producer 端的 CPU 开销。

2）增加分区

3）提高消费端吞吐量

- 调整 **fetch.max.bytes** 大小，默认是 50m。
- 调整 **max.poll.records** 大小，默认是 500 条。 

4）增加下游消费者处理能力



## 数据精准一次

1）生产者角度

- acks 设置为-1 (acks=-1)。
- 幂等性(enable.idempotence = true) + 事务 。

2）broke服务端角度

- 分区副本大于等于 2 (--replication-factor 2)。
-  ISR 里应答的最小副本数量大于等于 2 (min.insync.replicas = 2)。

3）消费者角度

- 事务 + 手动提交 offset (enable.auto.commit = false)。
- 消费者输出的目的地必须支持事务(MySQL、Kafka)。



## 合理设置分区数

1）创建一个只有 1 个分区的 topic。

2）测试这个 topic 的 producer 吞吐量和 consumer 吞吐量。 

3）假设他们的值分别是 Tp 和 Tc，单位可以是 MB/s。 

4）然后假设总的目标吞吐量是 Tt，那么分区数 = Tt / min(Tp，Tc)。 

例如:producer 吞吐量 = 20m/s;consumer 吞吐量 = 50m/s，期望吞吐量 100m/s; 分区数 = 100 / 20 = 5 分区，分区数一般设置为:3-10 个分区数不是越多越好，也不是越少越好，**需要搭建完集群，进行压测，再灵活调整分区个数。**



## 单条日志大于 1M

- **message.max.bytes**：默认 1m，broker 端接收每个批次消息最大值。
- **max.request.size**：默认 1m，生产者发往 broker 每个请求消息最大值。针对 topic 级别设置消息体的大小。 				
- **replica.fetch.max.bytes**：默认 1m，副本同步数据，每个批次消息最大值。
- **fetch.max.bytes**：**默认Default: 52428800(50m)。**消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值 (50m)仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受 message.max.bytes (broker config) or max.message.bytes (topic config)影响。



## 服务器宕机

在生产环境中，如果某个 Kafka 节点挂掉。 正常处理办法:

1. 先尝试重新启动一下，如果能启动正常，那直接解决。 
2. 如果重启不行，考虑增加内存、增加 CPU、网络带宽。
3. 如果将 kafka 整个节点误删除，如果副本数大于等于 2，可以按照服役新节点的方式重新服役一个新节点，并执行负载均衡。



# 集群压力测试

用 Kafka 官方自带的脚本，对 Kafka 进行压测。  

- 生产者压测:kafka-producer-perf-test.sh
- 消费者压测:kafka-consumer-perf-test.sh

## **Producer** 压力测试

1）创建一个 test topic，设置为 3 个分区 3 个副本

```shell
bin/kafka-topics.sh --bootstrap- server hadoop102:9092 --create --replication-factor 3 -- partitions 3 --topic test
```

2）在/opt/module/kafka/bin 目录下面有这两个压测脚本。

- record-size是一条信息有多大，单位是字节，本次测试设置为1k。
- num-records是总共发送多少条信息，本次测试设置为100万条。 
- throughput 是每秒多少条信息，设成-1，表示不限流，尽可能快的生产数据，可测出生产者最大吞吐量。本次实验设置为每秒钟 1 万条。
- producer-props 后面可以配置生产者相关参数，batch.size配置为16k。

```shell
bin/kafka-producer-perf-test.sh -- topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=16384 linger.ms=0
```

压测结果：

```shell
ap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092  batch.size=16384 linger.ms=0
37021 records sent, 7401.2 records/sec (7.23 MB/sec), 1136.0 ms avg latency, 1453.0 ms max latency.
50535 records sent, 10107.0 records/sec (9.87 MB/sec), 1199.5 ms avg latency, 1404.0 ms max latency.
47835 records sent, 9567.0 records/sec (9.34 MB/sec), 1350.8 ms avg latency, 1570.0 ms max latency.
... ...
42390 records sent, 8444.2 records/sec (8.25 MB/sec), 3372.6 ms avg latency, 4008.0 ms max latency.
37800 records sent, 7558.5 records/sec (7.38 MB/sec), 4079.7 ms avg latency, 4758.0 ms max latency.
33570 records sent, 6714.0 records/sec (6.56 MB/sec), 4549.0 ms avg latency, 5049.0 ms max latency.
1000000 records sent, 9180.713158 records/sec (⭐️8.97 MB/sec), 1894.78 ms avg latency, 5049.00 ms max latency, 1335 ms 50th, 4128 ms 95th, 4719 ms 99th, 5030 ms 99.9th.
```

3）调整不同参数再次压测

```shell
# batch.size 默认值是 16k。本次实验 batch.size 设置为 32k。
bin/kafka-producer-perf-test.sh -- topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=32768 linger.ms=0
# linger.ms 默认是 0ms。本次实验 linger.ms 设置为 50ms。
bin/kafka-producer-perf-test.sh -- topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=4096 linger.ms=50
# 调整压缩方式 默认的压缩方式是 none。本次实验 compression.type 设置为 snappy/zstd/gzip/lz4
bin/kafka-producer-perf-test.sh -- topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=4096 linger.ms=50 compression.type=snappy/zstd/gzip/lz4
# 默认生产者端缓存大小 32m。本次实验 buffer.memory 设置为 64m。
bin/kafka-producer-perf-test.sh -- topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=4096 linger.ms=50 buffer.memory=67108864
```



##  **Consumer** 压力测试

1）修改/opt/module/kafka/config/consumer.properties 文件中的一次拉取条数为 500

```shell
max.poll.records=500
```

2）消费 100 万条日志进行压测

- --bootstrap-server指定Kafka集群地址
- --topic 指定 topic 的名称
- --messages总共要消费的消息个数。本次实验100万条。

```shell
bin/kafka-consumer-perf-test.sh -- bootstrap-server hadoop102:9092,hadoop103:9092,hadoop104:9092 - -topic test --messages 1000000 --consumer.config config/consumer.properties
```

3）修改/opt/module/kafka/config/consumer.properties 文件中的一次拉取条数为 2000。再次执行

```shell
bin/kafka-consumer-perf-test.sh -- broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 -- topic test --messages 1000000 --consumer.config config/consumer.properties
```

4）调整 fetch.max.bytes 大小为 100m

```shell
# 修改/opt/module/kafka/config/consumer.properties 文件中的拉取一批数据大小 100m
fetch.max.bytes=104857600
# 再次执行
bin/kafka-consumer-perf-test.sh -- broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 -- topic test --messages 1000000 --consumer.config config/consumer.properties
```

